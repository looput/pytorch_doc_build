


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Quantization &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/quantization.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.random" href="random.html" />
    <link rel="prev" title="torch.optim" href="optim.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.1.0 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/quantization.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="__config__.html">torch.__config__</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Quantization</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/quantization.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="quantization">
<h1>Quantization<a class="headerlink" href="#quantization" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction-to-quantization">
<h2>Introduction to Quantization<a class="headerlink" href="#introduction-to-quantization" title="Permalink to this headline">¶</a></h2>
<p>Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than
floating point precision. A quantized model executes some or all of the operations on tensors with
integers rather than floating point values. This allows for a more
compact model representation and the use of high performance vectorized
operations on many hardware platforms. PyTorch supports INT8
quantization compared to typical FP32 models allowing for a 4x reduction in the model size and
a 4x reduction in memory bandwidth requirements.  Hardware support for  INT8 computations
is typically 2 to 4 times faster compared to FP32 compute. Quantization is primarily a technique
to speed up inference and only the forward pass is supported for quantized operators.</p>
<p>PyTorch supports multiple approaches to quantizing a deep learning model. In most cases the model is trained
in FP32 and then the model is converted to INT8. In addition, PyTorch also supports quantization aware
training, which models quantization errors in both the forward and backward passes using fake-quantization
modules. Note that the entire computation is carried out in floating point. At the end of quantization aware
training, PyTorch provides conversion functions to convert the trained model into lower precision.</p>
<p>At lower level, PyTorch provides a way to represent quantized tensors and
perform operations with them. They can be used to directly construct models that
perform all or part of the computation in lower precision. Higher-level APIs are
provided that incorporate typical workflows of converting FP32 model to lower
precision with minimal accuracy loss.</p>
</div>
<div class="section" id="quantized-tensors">
<h2>Quantized Tensors<a class="headerlink" href="#quantized-tensors" title="Permalink to this headline">¶</a></h2>
<p>PyTorch supports both per tensor and per channel asymmetric linear
quantization. Per tensor means that all the values within the tensor are
scaled the same way. Per channel means that for each dimension, typically
the channel dimension of a tensor, the values
in the tensor are scaled and offset by a different value (effectively
the scale and offset become vectors). This allows for lesser error in converting tensors
to quantized values.</p>
<p>The mapping is performed by converting the floating point tensors using</p>
<a class="reference internal image-reference" href="_images/math-quantizer-equation.png"><img alt="_images/math-quantizer-equation.png" src="_images/math-quantizer-equation.png" style="width: 40%;" /></a>
<p>Note that, we ensure that zero in floating point is represented with no error after quantization,
thereby ensuring that operations like padding do not cause additional quantization error.</p>
<p>In order to do quantization in PyTorch, we need to be able to represent
quantized data in Tensors. A Quantized Tensor allows for storing
quantized data (represented as int8/uint8/int32) along with quantization
parameters like scale and zero_point. Quantized Tensors allow for many
useful operations making quantized arithmetic easy, in addition to
allowing for serialization of data in a quantized format.</p>
</div>
<div class="section" id="operation-coverage">
<h2>Operation coverage<a class="headerlink" href="#operation-coverage" title="Permalink to this headline">¶</a></h2>
<p>Quantized Tensors support a limited subset of data manipulation methods of the regular
full-precision tensor. (see list below)</p>
<p>For NN operators included in PyTorch, we restrict support to:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>8 bit weights (data_type = qint8)</p></li>
<li><p>8 bit activations (data_type = quint8)</p></li>
</ol>
</div></blockquote>
<p>Note that operator implementations currently only
support per channel quantization for weights of the <strong>conv</strong> and <strong>linear</strong>
operators. Furthermore the minimum and the maximum of the input data is
mapped linearly to the minimum and the maximum of the quantized data
type such that zero is represented with no quantization error.</p>
<p>Additional data types and quantization schemes can be implemented through
the <a class="reference external" href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">custom operator mechanism</a>.</p>
<p>Many operations for quantized tensors are available under the same API as full
float version in <code class="docutils literal notranslate"><span class="pre">torch</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>. Quantized version of NN modules that
perform re-quantization are available in <code class="docutils literal notranslate"><span class="pre">torch.nn.quantized</span></code>. Those
operations explicitly take output quantization parameters (scale and zero_point) in
the operation signature.</p>
<p>In addition, we also support fused versions corresponding to common fusion patterns that impact quantization at:
torch.nn.intrinsic.quantized.</p>
<p>For quantization aware training, we support modules prepared for quantization aware training at
torch.nn.qat and torch.nn.intrinsic.qat</p>
<p>Current quantized operation list is sufficient to cover typical CNN and RNN
models:</p>
<div class="section" id="quantized-torch-tensor-operations">
<h3>Quantized <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> operations<a class="headerlink" href="#quantized-torch-tensor-operations" title="Permalink to this headline">¶</a></h3>
<p>Operations that are available from the <code class="docutils literal notranslate"><span class="pre">torch</span></code> namespace or as methods on Tensor for quantized tensors:</p>
<ul class="simple">
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">quantize_per_tensor()</span></code> - Convert float tensor to quantized tensor with per-tensor scale and zero point</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">quantize_per_channel()</span></code> - Convert float tensor to quantized tensor with per-channel scale and zero point</p></li>
<li><p>View-based operations like <a class="reference internal" href="tensors.html#torch.Tensor.view" title="torch.Tensor.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">view()</span></code></a>, <a class="reference internal" href="tensors.html#torch.Tensor.as_strided" title="torch.Tensor.as_strided"><code class="xref py py-meth docutils literal notranslate"><span class="pre">as_strided()</span></code></a>, <a class="reference internal" href="tensors.html#torch.Tensor.expand" title="torch.Tensor.expand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">expand()</span></code></a>, <a class="reference internal" href="tensors.html#torch.Tensor.flatten" title="torch.Tensor.flatten"><code class="xref py py-meth docutils literal notranslate"><span class="pre">flatten()</span></code></a>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">slice()</span></code>, python-style indexing, etc - work as on regular tensor (if quantization is not per-channel)</p></li>
<li><dl class="simple">
<dt>Comparators</dt><dd><ul>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.ne" title="torch.Tensor.ne"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ne()</span></code></a> — Not equal</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.eq" title="torch.Tensor.eq"><code class="xref py py-meth docutils literal notranslate"><span class="pre">eq()</span></code></a> — Equal</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.ge" title="torch.Tensor.ge"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ge()</span></code></a> — Greater or equal</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.le" title="torch.Tensor.le"><code class="xref py py-meth docutils literal notranslate"><span class="pre">le()</span></code></a> — Less or equal</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.gt" title="torch.Tensor.gt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">gt()</span></code></a> — Greater</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.lt" title="torch.Tensor.lt"><code class="xref py py-meth docutils literal notranslate"><span class="pre">lt()</span></code></a> — Less</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.copy_" title="torch.Tensor.copy_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">copy_()</span></code></a> — Copies src to self in-place</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.clone" title="torch.Tensor.clone"><code class="xref py py-meth docutils literal notranslate"><span class="pre">clone()</span></code></a> —  Returns a deep copy of the passed-in tensor</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.dequantize" title="torch.Tensor.dequantize"><code class="xref py py-meth docutils literal notranslate"><span class="pre">dequantize()</span></code></a> — Convert quantized tensor to float tensor</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.equal" title="torch.Tensor.equal"><code class="xref py py-meth docutils literal notranslate"><span class="pre">equal()</span></code></a> — Compares two tensors, returns true if quantization parameters and all integer elements are the same</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.int_repr" title="torch.Tensor.int_repr"><code class="xref py py-meth docutils literal notranslate"><span class="pre">int_repr()</span></code></a> — Prints the underlying integer representation of the quantized tensor</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.max" title="torch.Tensor.max"><code class="xref py py-meth docutils literal notranslate"><span class="pre">max()</span></code></a> — Returns the maximum value of the tensor (reduction only)</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.mean" title="torch.Tensor.mean"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mean()</span></code></a> — Mean function. Supported variants: reduction, dim, out</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.min" title="torch.Tensor.min"><code class="xref py py-meth docutils literal notranslate"><span class="pre">min()</span></code></a> — Returns the minimum value of the tensor (reduction only)</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.q_scale" title="torch.Tensor.q_scale"><code class="xref py py-meth docutils literal notranslate"><span class="pre">q_scale()</span></code></a> — Returns the scale of the per-tensor quantized tensor</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.q_zero_point" title="torch.Tensor.q_zero_point"><code class="xref py py-meth docutils literal notranslate"><span class="pre">q_zero_point()</span></code></a> — Returns the zero_point of the per-tensor quantized zero point</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">q_per_channel_scales()</span></code> — Returns the scales of the per-channel quantized tensor</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">q_per_channel_zero_points()</span></code> — Returns the zero points of the per-channel quantized tensor</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">q_per_channel_axis()</span></code> — Returns the channel axis of the per-channel quantized tensor</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">relu()</span></code> — Rectified linear unit (copy)</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">relu_()</span></code> — Rectified linear unit (inplace)</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.resize_" title="torch.Tensor.resize_"><code class="xref py py-meth docutils literal notranslate"><span class="pre">resize_()</span></code></a> — In-place resize</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.sort" title="torch.Tensor.sort"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sort()</span></code></a> — Sorts the tensor</p></li>
<li><p><a class="reference internal" href="tensors.html#torch.Tensor.topk" title="torch.Tensor.topk"><code class="xref py py-meth docutils literal notranslate"><span class="pre">topk()</span></code></a> — Returns k largest values of a tensor</p></li>
</ul>
</div>
<div class="section" id="torch-nn-intrinsic">
<h3><code class="docutils literal notranslate"><span class="pre">torch.nn.intrinsic</span></code><a class="headerlink" href="#torch-nn-intrinsic" title="Permalink to this headline">¶</a></h3>
<p>Fused modules are provided for common patterns in CNNs. Combining several operations together (like convolution and relu) allows for better quantization accuracy</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.nn.intrinsic</span></code> — float versions of the modules, can be swapped with quantized version 1 to 1</dt><dd><ul>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvBn2d</span></code> — Conv2d + BatchNorm</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvBnReLU2d</span></code> — Conv2d + BatchNorm + ReLU</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvReLU2d</span></code> — Conv2d + Relu</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearReLU</span></code> — Linear + ReLU</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.nn.intrinsic.qat</span></code> — versions of layers for quantization-aware training</dt><dd><ul>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvBn2d</span></code> — Conv2d + BatchNorm</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvBnReLU2d</span></code> — Conv2d + BatchNorm + ReLU</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvReLU2d</span></code> — Conv2d + ReLU</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearReLU</span></code> — Linear + ReLU</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.nn.intrinsic.quantized</span></code> — quantized version of fused layers for inference (no BatchNorm variants as it’s usually folded into convolution for inference)</dt><dd><ul>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearReLU</span></code> — Linear + ReLU</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvReLU2d</span></code> — 2D Convolution + ReLU</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="torch-nn-qat">
<h3><code class="docutils literal notranslate"><span class="pre">torch.nn.qat</span></code><a class="headerlink" href="#torch-nn-qat" title="Permalink to this headline">¶</a></h3>
<p>Layers for the quantization-aware training</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code> — Linear (fully-connected) layer</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code> — 2D convolution</p></li>
</ul>
</div>
<div class="section" id="torch-quantization">
<h3><code class="docutils literal notranslate"><span class="pre">torch.quantization</span></code><a class="headerlink" href="#torch-quantization" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><dl class="simple">
<dt>Functions for quantization</dt><dd><ul>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">add_observer_()</span></code> — Adds observer for the leaf modules (if quantization configuration is provided)</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">add_quant_dequant()</span></code>— Wraps the leaf child module using <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantWrapper</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">convert()</span></code> — Converts float module with observers into its quantized counterpart. Must have quantization configuration</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">get_observer_dict()</span></code> — Traverses the module children and collects all observers into a <code class="docutils literal notranslate"><span class="pre">dict</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">prepare()</span></code> — Prepares a copy of a model for quantization</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">prepare_qat()</span></code> — Prepares a copy of a model for quantization aware training</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">propagate_qconfig_()</span></code> — Propagates quantization configurations through the module hierarchy and assign them to each leaf module</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">quantize()</span></code> — Converts a float module to quantized version</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">quantize_dynamic()</span></code> — Converts a float module to dynamically quantized version</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">quantize_qat()</span></code>— Converts a float module to quantized version used in quantization aware training</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">swap_module()</span></code> — Swaps the module with its quantized counterpart (if quantizable and if it has an observer)</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">default_eval_fn()</span></code> — Default evaluation function used by the <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.quantization.quantize()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">fuse_modules()</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">FakeQuantize</span></code> — Module for simulating the quantization/dequantization at training time</p></li>
<li><dl class="simple">
<dt>Default Observers. The rest of observers are available from <code class="docutils literal notranslate"><span class="pre">torch.quantization.observer</span></code></dt><dd><ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_observer</span></code> — Same as <code class="docutils literal notranslate"><span class="pre">MinMaxObserver.with_args(reduce_range=True)</span></code></p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_weight_observer</span></code> — Same as <code class="docutils literal notranslate"><span class="pre">MinMaxObserver.with_args(dtype=torch.qint8,</span> <span class="pre">qscheme=torch.per_tensor_symmetric)</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Observer</span></code> — Abstract base class for observers</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Quantization configurations</dt><dd><ul>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">QConfig</span></code> — Quantization configuration class</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_qconfig</span></code> — Same as <code class="docutils literal notranslate"><span class="pre">QConfig(activation=default_observer,</span> <span class="pre">weight=default_weight_observer)</span></code> (See <code class="xref py py-class docutils literal notranslate"><span class="pre">QConfig</span></code>)</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_qat_qconfig</span></code> — Same as <code class="docutils literal notranslate"><span class="pre">QConfig(activation=default_fake_quant,</span> <span class="pre">weight=default_weight_fake_quant)</span></code> (See <code class="xref py py-class docutils literal notranslate"><span class="pre">QConfig</span></code>)</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_dynamic_qconfig</span></code> — Same as <code class="docutils literal notranslate"><span class="pre">QConfigDynamic(weight=default_weight_observer)</span></code> (See <code class="xref py py-class docutils literal notranslate"><span class="pre">QConfigDynamic</span></code>)</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">float16_dynamic_qconfig</span></code> — Same as <code class="docutils literal notranslate"><span class="pre">QConfigDynamic(weight=NoopObserver.with_args(dtype=torch.float16))</span></code> (See <code class="xref py py-class docutils literal notranslate"><span class="pre">QConfigDynamic</span></code>)</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Stubs</dt><dd><ul>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DeQuantStub</span></code> - placeholder module for dequantize() operation in float-valued models</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantStub</span></code> - placeholder module for quantize() operation in float-valued models</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantWrapper</span></code> — wraps the module to be quantized. Inserts the <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantStub</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">DeQuantStub</span></code></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Observers for computing the quantization parameters</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">MinMaxObserver</span></code> — Derives the quantization parameters from the running minimum and maximum of the observed tensor inputs (per tensor variant)</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">MovingAverageObserver</span></code> — Derives the quantization parameters from the running averages of the minimums and maximums of the observed tensor inputs (per tensor variant)</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">PerChannelMinMaxObserver</span></code>— Derives the quantization parameters from the running minimum and maximum of the observed tensor inputs (per channel variant)</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">MovingAveragePerChannelMinMaxObserver</span></code> — Derives the quantization parameters from the running averages of the minimums and maximums of the observed tensor inputs (per channel variant)</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">HistogramObserver</span></code> — Derives the quantization parameters by creating a histogram of running minimums and maximums.</p></li>
<li><dl class="simple">
<dt>Observers that do not compute the quantization parameters:</dt><dd><ul>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">RecordingObserver</span></code> — Records all incoming tensors. Used for debugging only.</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">NoopObserver</span></code> — Pass-through observer. Used for situation when there are no quantization parameters (i.e. quantization to <code class="docutils literal notranslate"><span class="pre">float16</span></code>)</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="torch-nn-quantized">
<h3><code class="docutils literal notranslate"><span class="pre">torch.nn.quantized</span></code><a class="headerlink" href="#torch-nn-quantized" title="Permalink to this headline">¶</a></h3>
<p>Quantized version of standard NN layers.</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Quantize</span></code> — Quantization layer, used to automatically replace <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantStub</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DeQuantize</span></code> — Dequantization layer, used to replace <code class="xref py py-class docutils literal notranslate"><span class="pre">DeQuantStub</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">FloatFunctional</span></code> — Wrapper class to make stateless float operations stateful so that they can be replaced with quantized versions</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">QFunctional</span></code> — Wrapper class for quantized versions of stateless operations like <code class="docutils literal notranslate"><span class="pre">`torch.add</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code> — 2D convolution</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code> — Linear (fully-connected) layer</p></li>
<li><p><a class="reference internal" href="nn.html#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a> — 2D max pooling</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU</span></code> — Rectified linear unit</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU6</span></code> — Rectified linear unit with cut-off at quantized representation of 6</p></li>
</ul>
</div>
<div class="section" id="torch-nn-quantized-dynamic">
<h3><code class="docutils literal notranslate"><span class="pre">torch.nn.quantized.dynamic</span></code><a class="headerlink" href="#torch-nn-quantized-dynamic" title="Permalink to this headline">¶</a></h3>
<p>Layers used in dynamically quantized models (i.e. quantized only on weights)</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code> — Linear (fully-connected) layer</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTM</span></code> — Long-Short Term Memory RNN module</p></li>
</ul>
</div>
<div class="section" id="torch-nn-quantized-functional">
<h3><code class="docutils literal notranslate"><span class="pre">torch.nn.quantized.functional</span></code><a class="headerlink" href="#torch-nn-quantized-functional" title="Permalink to this headline">¶</a></h3>
<p>Functional versions of quantized NN layers (many of them accept explicit quantization output parameters)</p>
<ul class="simple">
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">adaptive_avg_pool2d()</span></code> — 2D adaptive average pooling</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">avg_pool2d()</span></code> — 2D average pooling</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">conv2d()</span></code> — 2D convolution</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code> — Down-/up- sampler</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">linear()</span></code> — Linear (fully-connected) op</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">max_pool2d()</span></code> — 2D max pooling</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">relu()</span></code> — Rectified linear unit</p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">upsample()</span></code> — Upsampler. Will be deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">upsample_bilinear()</span></code> — Bilenear upsampler. Will be deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">upsample_nearest()</span></code> — Nearest neighbor upsampler. Will be deprecated in favor of <code class="xref py py-func docutils literal notranslate"><span class="pre">interpolate()</span></code></p></li>
</ul>
</div>
<div class="section" id="quantized-dtypes-and-quantization-schemes">
<h3>Quantized dtypes and quantization schemes<a class="headerlink" href="#quantized-dtypes-and-quantization-schemes" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><dl class="simple">
<dt><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qscheme</span></code> — Type to describe the quantization scheme of a tensor. Supported types:</dt><dd><ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_tensor_affine</span></code> — per tensor, asymmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_channel_affine</span></code> — per channel, asymmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_tensor_symmetric</span></code> — per tensor, symmetric</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.per_channel_symmetric</span></code> — per tensor, symmetric</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.dtype</span></code> — Type to describe the data. Supported types:</dt><dd><ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.quint8</span></code> — 8-bit unsigned integer</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qint8</span></code> — 8-bit signed integer</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">torch.qint32</span></code> — 32-bit signed integer</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
</div>
<div class="section" id="quantization-workflows">
<h2>Quantization Workflows<a class="headerlink" href="#quantization-workflows" title="Permalink to this headline">¶</a></h2>
<p>PyTorch provides three approaches to quantize models.</p>
<ol class="arabic">
<li><p>Post Training Dynamic Quantization: This is the simplest to apply form of
quantization where the weights are quantized ahead of time but the
activations are dynamically quantized  during inference. This is used
for situations where the model execution time is dominated by loading
weights from memory rather than computing the matrix multiplications.
This is true for for LSTM and Transformer type models with small
batch size. Applying dynamic quantization to a whole model can be
done with a single call to <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.quantization.quantize_dynamic()</span></code>.
See the <a class="reference external" href="https://pytorch.org/tutorials/#quantization-experimental">quantization tutorials</a></p></li>
<li><p>Post Training Static Quantization: This is the most commonly used form of
quantization where the weights are quantized ahead of time and the
scale factor and bias for the activation tensors is pre-computed
based on observing the behavior of the model during a calibration
process. Post Training Quantization is typically when both memory bandwidth and compute
savings are important with CNNs being a typical use case.
The general process for doing post training quantization is:</p>
<ol class="arabic simple">
<li><p>Prepare the model:
a. Specify where the activations are quantized and dequantized explicitly by adding QuantStub and DeQuantStub modules.
b. Ensure that modules are not reused.
c. Convert any operations that require requantization into modules</p></li>
<li><p>Fuse operations like conv + relu or conv+batchnorm + relu together to improve both model accuracy and performance.</p></li>
<li><p>Specify the configuration of the quantization methods ‘97 such as
selecting symmetric or asymmetric quantization and MinMax or
L2Norm calibration techniques.</p></li>
<li><p>Use the <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.quantization.prepare()</span></code> to insert modules
that will observe activation tensors during calibration</p></li>
<li><p>Calibrate the model by running inference against a calibration
dataset</p></li>
<li><p>Finally, convert the model itself with the
torch.quantization.convert() method. This does several things: it
quantizes the weights, computes and stores the scale and bias
value to be used each activation tensor, and replaces key
operators quantized implementations.</p></li>
</ol>
<p>See the <a class="reference external" href="https://pytorch.org/tutorials/#quantization_experimental">quantization tutorials</a></p>
</li>
<li><p>Quantization Aware Training: In the rare cases where post training
quantization does not provide adequate accuracy training can be done
with simulated quantization using the <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.quantization.FakeQuantize</span></code>. Computations
will take place in FP32 but with values clamped and rounded to
simulate the effects of INT8 quantization. The sequence of steps is
very similar.</p>
<ol class="arabic simple">
<li><p>Steps (1) and (2) are identical.</p></li>
</ol>
<ol class="arabic simple" start="3">
<li><p>Specify the configuration of the fake quantization methods ‘97 such as
selecting symmetric or asymmetric quantization and MinMax or Moving Average
or L2Norm calibration techniques.</p></li>
<li><p>Use the <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.quantization.prepare_qat()</span></code> to insert modules
that will simulate quantization during training.</p></li>
<li><p>Train or fine tune the model.</p></li>
<li><p>Identical to step (6) for post training quantization</p></li>
</ol>
<p>See the <a class="reference external" href="https://pytorch.org/tutorials/#quantization_experimental">quantization tutorials</a></p>
</li>
</ol>
<p>While default implementations of observers to select the scale factor and bias
based on observed tensor data are provided, developers can provide their own
quantization functions. Quantization can be applied selectively to different
parts of the model or configured differently for different parts of the model.</p>
<p>We also provide support for per channel quantization for <strong>conv2d()</strong>
and <strong>linear()</strong></p>
<p>Quantization workflows work by adding (e.g. adding observers as
<code class="docutils literal notranslate"><span class="pre">.observer</span></code> submodule) or replacing (e.g. converting <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> to
<code class="docutils literal notranslate"><span class="pre">nn.quantized.Conv2d</span></code>) submodules in the model’s module hierarchy. It
means that the model stays a regular <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>-based instance throughout the
process and thus can work with the rest of PyTorch APIs.</p>
</div>
<div class="section" id="model-preparation-for-quantization">
<h2>Model Preparation for Quantization<a class="headerlink" href="#model-preparation-for-quantization" title="Permalink to this headline">¶</a></h2>
<p>It is necessary to currently make some modifications to the model definition
prior to quantization. This is because currently quantization works on a module
by module basis. Specifically, for all quantization techniques, the user needs to:</p>
<ol class="arabic simple">
<li><p>Convert any operations that require output requantization (and thus have additional parameters) from functionals to module form.</p></li>
<li><p>Specify which parts of the model need to be quantized either by assigning <code class="docutils literal notranslate"><span class="pre">`.qconfig</span></code> attributes on submodules or by specifying <code class="docutils literal notranslate"><span class="pre">qconfig_dict</span></code></p></li>
</ol>
<p>For static quantization techniques which quantize activations, the user needs to do the following in addition:</p>
<ol class="arabic simple">
<li><p>Specify where activations are quantized and de-quantized. This is done using <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantStub</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">DeQuantStub</span></code> modules.</p></li>
<li><p>Use <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.quantized.FloatFunctional</span></code> to wrap tensor operations that require special handling for quantization into modules. Examples
are operations like <code class="docutils literal notranslate"><span class="pre">add</span></code> and <code class="docutils literal notranslate"><span class="pre">cat</span></code> which require special handling to determine output quantization parameters.</p></li>
<li><p>Fuse modules: combine operations/modules into a single module to obtain higher accuracy and performance. This is done using the
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.quantization.fuse_modules()</span></code> API, which takes in lists of modules to be fused. We currently support the following fusions:
[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]</p></li>
</ol>
</div>
<div class="section" id="id3">
<h2>torch.quantization<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>This module implements the functions you call
directly to convert your model from FP32 to quantized form. For
example the <code class="xref py py-func docutils literal notranslate"><span class="pre">prepare()</span></code> is used in post training
quantization to prepares your model for the calibration step and
<code class="xref py py-func docutils literal notranslate"><span class="pre">convert()</span></code> actually converts the weights to int8 and
replaces the operations with their quantized counterparts. There are
other helper functions for things like quantizing the input to your
model and performing critical fusions like conv+relu.</p>
<div class="section" id="top-level-quantization-apis">
<h3>Top-level quantization APIs<a class="headerlink" href="#top-level-quantization-apis" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="preparing-model-for-quantization">
<h3>Preparing model for quantization<a class="headerlink" href="#preparing-model-for-quantization" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="utility-functions">
<h3>Utility functions<a class="headerlink" href="#utility-functions" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="observers">
<h3>Observers<a class="headerlink" href="#observers" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="debugging-utilities">
<h3>Debugging utilities<a class="headerlink" href="#debugging-utilities" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="torch-nn-instrinsic">
<h2>torch.nn.instrinsic<a class="headerlink" href="#torch-nn-instrinsic" title="Permalink to this headline">¶</a></h2>
<p>This module implements the combined (fused) modules conv + relu which can be then quantized.</p>
<div class="section" id="convbn2d">
<h3>ConvBn2d<a class="headerlink" href="#convbn2d" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="convbnrelu2d">
<h3>ConvBnReLU2d<a class="headerlink" href="#convbnrelu2d" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="convrelu2d">
<h3>ConvReLU2d<a class="headerlink" href="#convrelu2d" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="linearrelu">
<h3>LinearReLU<a class="headerlink" href="#linearrelu" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="torch-nn-instrinsic-qat">
<h2>torch.nn.instrinsic.qat<a class="headerlink" href="#torch-nn-instrinsic-qat" title="Permalink to this headline">¶</a></h2>
<p>This module implements the versions of those fused operations needed for quantization aware training.</p>
<div class="section" id="id4">
<h3>ConvBn2d<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id5">
<h3>ConvBnReLU2d<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id6">
<h3>ConvReLU2d<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id7">
<h3>LinearReLU<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="torch-nn-intrinsic-quantized">
<h2>torch.nn.intrinsic.quantized<a class="headerlink" href="#torch-nn-intrinsic-quantized" title="Permalink to this headline">¶</a></h2>
<p>This module implements the quantized implementations of fused operations like conv + relu.</p>
<div class="section" id="id8">
<h3>ConvReLU2d<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id9">
<h3>LinearReLU<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="id10">
<h2>torch.nn.qat<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<p>This module implements versions of the key nn modules <strong>Conv2d()</strong> and <strong>Linear()</strong> which
run in FP32 but with rounding applied to simulate the effect of INT8 quantization.</p>
<div class="section" id="conv2d">
<h3>Conv2d<a class="headerlink" href="#conv2d" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="linear">
<h3>Linear<a class="headerlink" href="#linear" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="id11">
<h2>torch.nn.quantized<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<p>This module implements the quantized versions of the nn layers such as <strong>Conv2d</strong> and <strong>ReLU</strong>.</p>
<div class="section" id="functional-interface">
<h3>Functional interface<a class="headerlink" href="#functional-interface" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="relu">
<h3>ReLU<a class="headerlink" href="#relu" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="relu6">
<h3>ReLU6<a class="headerlink" href="#relu6" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id12">
<h3>Conv2d<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="floatfunctional">
<h3>FloatFunctional<a class="headerlink" href="#floatfunctional" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="qfunctional">
<h3>QFunctional<a class="headerlink" href="#qfunctional" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="quantize">
<h3>Quantize<a class="headerlink" href="#quantize" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="dequantize">
<h3>DeQuantize<a class="headerlink" href="#dequantize" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id13">
<h3>Linear<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="id14">
<h2>torch.nn.quantized.dynamic<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id15">
<h3>Linear<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="lstm">
<h3>LSTM<a class="headerlink" href="#lstm" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="random.html" class="btn btn-neutral float-right" title="torch.random" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="optim.html" class="btn btn-neutral" title="torch.optim" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Quantization</a><ul>
<li><a class="reference internal" href="#introduction-to-quantization">Introduction to Quantization</a></li>
<li><a class="reference internal" href="#quantized-tensors">Quantized Tensors</a></li>
<li><a class="reference internal" href="#operation-coverage">Operation coverage</a><ul>
<li><a class="reference internal" href="#quantized-torch-tensor-operations">Quantized <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> operations</a></li>
<li><a class="reference internal" href="#torch-nn-intrinsic"><code class="docutils literal notranslate"><span class="pre">torch.nn.intrinsic</span></code></a></li>
<li><a class="reference internal" href="#torch-nn-qat"><code class="docutils literal notranslate"><span class="pre">torch.nn.qat</span></code></a></li>
<li><a class="reference internal" href="#torch-quantization"><code class="docutils literal notranslate"><span class="pre">torch.quantization</span></code></a></li>
<li><a class="reference internal" href="#torch-nn-quantized"><code class="docutils literal notranslate"><span class="pre">torch.nn.quantized</span></code></a></li>
<li><a class="reference internal" href="#torch-nn-quantized-dynamic"><code class="docutils literal notranslate"><span class="pre">torch.nn.quantized.dynamic</span></code></a></li>
<li><a class="reference internal" href="#torch-nn-quantized-functional"><code class="docutils literal notranslate"><span class="pre">torch.nn.quantized.functional</span></code></a></li>
<li><a class="reference internal" href="#quantized-dtypes-and-quantization-schemes">Quantized dtypes and quantization schemes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantization-workflows">Quantization Workflows</a></li>
<li><a class="reference internal" href="#model-preparation-for-quantization">Model Preparation for Quantization</a></li>
<li><a class="reference internal" href="#id3">torch.quantization</a><ul>
<li><a class="reference internal" href="#top-level-quantization-apis">Top-level quantization APIs</a></li>
<li><a class="reference internal" href="#preparing-model-for-quantization">Preparing model for quantization</a></li>
<li><a class="reference internal" href="#utility-functions">Utility functions</a></li>
<li><a class="reference internal" href="#observers">Observers</a></li>
<li><a class="reference internal" href="#debugging-utilities">Debugging utilities</a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch-nn-instrinsic">torch.nn.instrinsic</a><ul>
<li><a class="reference internal" href="#convbn2d">ConvBn2d</a></li>
<li><a class="reference internal" href="#convbnrelu2d">ConvBnReLU2d</a></li>
<li><a class="reference internal" href="#convrelu2d">ConvReLU2d</a></li>
<li><a class="reference internal" href="#linearrelu">LinearReLU</a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch-nn-instrinsic-qat">torch.nn.instrinsic.qat</a><ul>
<li><a class="reference internal" href="#id4">ConvBn2d</a></li>
<li><a class="reference internal" href="#id5">ConvBnReLU2d</a></li>
<li><a class="reference internal" href="#id6">ConvReLU2d</a></li>
<li><a class="reference internal" href="#id7">LinearReLU</a></li>
</ul>
</li>
<li><a class="reference internal" href="#torch-nn-intrinsic-quantized">torch.nn.intrinsic.quantized</a><ul>
<li><a class="reference internal" href="#id8">ConvReLU2d</a></li>
<li><a class="reference internal" href="#id9">LinearReLU</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id10">torch.nn.qat</a><ul>
<li><a class="reference internal" href="#conv2d">Conv2d</a></li>
<li><a class="reference internal" href="#linear">Linear</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id11">torch.nn.quantized</a><ul>
<li><a class="reference internal" href="#functional-interface">Functional interface</a></li>
<li><a class="reference internal" href="#relu">ReLU</a></li>
<li><a class="reference internal" href="#relu6">ReLU6</a></li>
<li><a class="reference internal" href="#id12">Conv2d</a></li>
<li><a class="reference internal" href="#floatfunctional">FloatFunctional</a></li>
<li><a class="reference internal" href="#qfunctional">QFunctional</a></li>
<li><a class="reference internal" href="#quantize">Quantize</a></li>
<li><a class="reference internal" href="#dequantize">DeQuantize</a></li>
<li><a class="reference internal" href="#id13">Linear</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id14">torch.nn.quantized.dynamic</a><ul>
<li><a class="reference internal" href="#id15">Linear</a></li>
<li><a class="reference internal" href="#lstm">LSTM</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script type="text/javascript" src="_static/jquery.js"></script>
         <script type="text/javascript" src="_static/underscore.js"></script>
         <script type="text/javascript" src="_static/doctools.js"></script>
         <script type="text/javascript" src="_static/language_data.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>