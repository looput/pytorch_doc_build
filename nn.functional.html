


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.functional &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/nn.functional.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="torch.Tensor" href="tensors.html" />
    <link rel="prev" title="torch.nn" href="nn.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.1.0 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/nn.functional.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="__config__.html">torch.__config__</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torch.nn.functional</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/nn.functional.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="torch-nn-functional">
<h1>torch.nn.functional<a class="headerlink" href="#torch-nn-functional" title="Permalink to this headline">¶</a></h1>
<div class="section" id="convolution-functions">
<h2>Convolution functions<a class="headerlink" href="#convolution-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="conv1d">
<h3><span class="hidden-section">conv1d</span><a class="headerlink" href="#conv1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D convolution over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Conv1d" title="torch.nn.Conv1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or
a one-element tuple <cite>(sW,)</cite>. Default: 1</p></li>
<li><p><strong>padding</strong> – implicit paddings on both sides of the input. Can be a
single number or a one-element tuple <cite>(padW,)</cite>. Default: 0</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a one-element tuple <cite>(dW,)</cite>. Default: 1</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by
the number of groups. Default: 1</p></li>
<li><p><strong>padding_mode</strong> – the type of paddings applied to both sided can be: <cite>zeros</cite> or <cite>circular</cite>. Default: <cite>zeros</cite></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv2d">
<h3><span class="hidden-section">conv2d</span><a class="headerlink" href="#conv2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D convolution over an input image composed of several input
planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Conv2d" title="torch.nn.Conv2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias tensor of shape <span class="math">\((\text{out\_channels})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <cite>(sH, sW)</cite>. Default: 1</p></li>
<li><p><strong>padding</strong> – implicit paddings on both sides of the input. Can be a
single number or a tuple <cite>(padH, padW)</cite>. Default: 0</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dH, dW)</cite>. Default: 1</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
<li><p><strong>padding_mode</strong> – the type of paddings applied to both sided can be: <cite>zeros</cite> or <cite>circular</cite>. Default: <cite>zeros</cite></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv3d">
<h3><span class="hidden-section">conv3d</span><a class="headerlink" href="#conv3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>dilation=1</em>, <em>groups=1</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D convolution over an input image composed of several input
planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Conv3d" title="torch.nn.Conv3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iT , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kT , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias tensor of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <cite>(sT, sH, sW)</cite>. Default: 1</p></li>
<li><p><strong>padding</strong> – implicit paddings on both sides of the input. Can be a
single number or a tuple <cite>(padT, padH, padW)</cite>. Default: 0</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dT, dH, dW)</cite>. Default: 1</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by
the number of groups. Default: 1</p></li>
<li><p><strong>padding_mode</strong> – the type of paddings applied to both sided can be: <cite>zeros</cite> or <cite>circular</cite>. Default: <cite>zeros</cite></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">filters</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">33</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">filters</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose1d">
<h3><span class="hidden-section">conv_transpose1d</span><a class="headerlink" href="#conv-transpose1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose1d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D transposed convolution operator over an input signal
composed of several input planes, sometimes also called “deconvolution”.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sW,)</span></code>. Default: 1</p></li>
<li><p><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padW,)</span></code>. Default: 0</p></li>
<li><p><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple <code class="docutils literal notranslate"><span class="pre">(out_padW)</span></code>. Default: 0</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <code class="docutils literal notranslate"><span class="pre">(dW,)</span></code>. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose2d">
<h3><span class="hidden-section">conv_transpose2d</span><a class="headerlink" href="#conv-transpose2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose2d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D transposed convolution operator over an input image
composed of several input planes, sometimes also called “deconvolution”.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sH,</span> <span class="pre">sW)</span></code>. Default: 1</p></li>
<li><p><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padH,</span> <span class="pre">padW)</span></code>. Default: 0</p></li>
<li><p><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple <code class="docutils literal notranslate"><span class="pre">(out_padH,</span> <span class="pre">out_padW)</span></code>.
Default: 0</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <code class="docutils literal notranslate"><span class="pre">(dH,</span> <span class="pre">dW)</span></code>. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># With square kernels and equal stride</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="conv-transpose3d">
<h3><span class="hidden-section">conv_transpose3d</span><a class="headerlink" href="#conv-transpose3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.conv_transpose3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">conv_transpose3d</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em>, <em>stride=1</em>, <em>padding=0</em>, <em>output_padding=0</em>, <em>groups=1</em>, <em>dilation=1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.conv_transpose3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D transposed convolution operator over an input image
composed of several input planes, sometimes also called “deconvolution”</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code></a> for details and output shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iT , iH , iW)\)</span></p></li>
<li><p><strong>weight</strong> – filters of shape <span class="math">\((\text{in\_channels} , \frac{\text{out\_channels}}{\text{groups}} , kT , kH , kW)\)</span></p></li>
<li><p><strong>bias</strong> – optional bias of shape <span class="math">\((\text{out\_channels})\)</span>. Default: None</p></li>
<li><p><strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a
tuple <code class="docutils literal notranslate"><span class="pre">(sT,</span> <span class="pre">sH,</span> <span class="pre">sW)</span></code>. Default: 1</p></li>
<li><p><strong>padding</strong> – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding will be added to both
sides of each dimension in the input. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(padT,</span> <span class="pre">padH,</span> <span class="pre">padW)</span></code>. Default: 0</p></li>
<li><p><strong>output_padding</strong> – additional size added to one side of each dimension in the
output shape. Can be a single number or a tuple
<code class="docutils literal notranslate"><span class="pre">(out_padT,</span> <span class="pre">out_padH,</span> <span class="pre">out_padW)</span></code>. Default: 0</p></li>
<li><p><strong>groups</strong> – split input into groups, <span class="math">\(\text{in\_channels}\)</span> should be divisible by the
number of groups. Default: 1</p></li>
<li><p><strong>dilation</strong> – the spacing between kernel elements. Can be a single number or
a tuple <cite>(dT, dH, dW)</cite>. Default: 1</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="unfold">
<h3><span class="hidden-section">unfold</span><a class="headerlink" href="#unfold" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.unfold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">unfold</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#unfold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.unfold" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts sliding local blocks from an batched input tensor.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Currently, only 4-D input tensors (batched image-like tensors) are
supported.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>More than one element of the unfolded tensor may refer to a single
memory location. As a result, in-place operations (especially ones that
are vectorized) may result in incorrect behavior. If you need to write
to the tensor, please clone it first.</p>
</div>
<p>See <a class="reference internal" href="nn.html#torch.nn.Unfold" title="torch.nn.Unfold"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Unfold</span></code></a> for details</p>
</dd></dl>

</div>
<div class="section" id="fold">
<h3><span class="hidden-section">fold</span><a class="headerlink" href="#fold" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.fold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">fold</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em>, <em>kernel_size</em>, <em>dilation=1</em>, <em>padding=0</em>, <em>stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#fold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.fold" title="Permalink to this definition">¶</a></dt>
<dd><p>Combines an array of sliding local blocks into a large containing
tensor.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Currently, only 4-D output tensors (batched image-like tensors) are
supported.</p>
</div>
<p>See <a class="reference internal" href="nn.html#torch.nn.Fold" title="torch.nn.Fold"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Fold</span></code></a> for details</p>
</dd></dl>

</div>
</div>
<div class="section" id="pooling-functions">
<h2>Pooling functions<a class="headerlink" href="#pooling-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="avg-pool1d">
<h3><span class="hidden-section">avg_pool1d</span><a class="headerlink" href="#avg-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D average pooling over an input signal composed of several
input planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.AvgPool1d" title="torch.nn.AvgPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool1d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\((\text{minibatch} , \text{in\_channels} , iW)\)</span></p></li>
<li><p><strong>kernel_size</strong> – the size of the window. Can be a single number or a
tuple <cite>(kW,)</cite></p></li>
<li><p><strong>stride</strong> – the stride of the window. Can be a single number or a tuple
<cite>(sW,)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padW,)</cite>. Default: 0</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the
output shape. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># pool of square window of size=3, stride=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">tensor([[[ 2.,  4.,  6.]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="avg-pool2d">
<h3><span class="hidden-section">avg_pool2d</span><a class="headerlink" href="#avg-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 2D average-pooling operation in <span class="math">\(kH \times kW\)</span> regions by step size
<span class="math">\(sH \times sW\)</span> steps. The number of output features is equal to the number of
input planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.AvgPool2d" title="torch.nn.AvgPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool2d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor <span class="math">\((\text{minibatch} , \text{in\_channels} , iH , iW)\)</span></p></li>
<li><p><strong>kernel_size</strong> – size of the pooling region. Can be a single number or a
tuple <cite>(kH, kW)</cite></p></li>
<li><p><strong>stride</strong> – stride of the pooling operation. Can be a single number or a
tuple <cite>(sH, sW)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padH, padW)</cite>. Default: 0</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> in the formula
to compute the output shape. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="avg-pool3d">
<h3><span class="hidden-section">avg_pool3d</span><a class="headerlink" href="#avg-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.avg_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">avg_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>ceil_mode=False</em>, <em>count_include_pad=True</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.avg_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies 3D average-pooling operation in <span class="math">\(kT \times kH \times kW\)</span> regions by step
size <span class="math">\(sT \times sH \times sW\)</span> steps. The number of output features is equal to
<span class="math">\(\lfloor\frac{\text{input planes}}{sT}\rfloor\)</span>.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.AvgPool3d" title="torch.nn.AvgPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool3d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor <span class="math">\((\text{minibatch} , \text{in\_channels} , iT \times iH , iW)\)</span></p></li>
<li><p><strong>kernel_size</strong> – size of the pooling region. Can be a single number or a
tuple <cite>(kT, kH, kW)</cite></p></li>
<li><p><strong>stride</strong> – stride of the pooling operation. Can be a single number or a
tuple <cite>(sT, sH, sW)</cite>. Default: <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a
single number or a tuple <cite>(padT, padH, padW)</cite>, Default: 0</p></li>
<li><p><strong>ceil_mode</strong> – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> in the formula
to compute the output shape</p></li>
<li><p><strong>count_include_pad</strong> – when True, will include the zero-padding in the
averaging calculation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="max-pool1d">
<h3><span class="hidden-section">max_pool1d</span><a class="headerlink" href="#max-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool1d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.MaxPool1d" title="torch.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-pool2d">
<h3><span class="hidden-section">max_pool2d</span><a class="headerlink" href="#max-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool2d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.MaxPool2d" title="torch.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-pool3d">
<h3><span class="hidden-section">max_pool3d</span><a class="headerlink" href="#max-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_pool3d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.max_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D max pooling over an input signal composed of several input
planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool1d">
<h3><span class="hidden-section">max_unpool1d</span><a class="headerlink" href="#max-unpool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool1d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool1d</span></code>.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.MaxUnpool1d" title="torch.nn.MaxUnpool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool2d">
<h3><span class="hidden-section">max_unpool2d</span><a class="headerlink" href="#max-unpool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool2d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool2d</span></code>.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.MaxUnpool2d" title="torch.nn.MaxUnpool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="max-unpool3d">
<h3><span class="hidden-section">max_unpool3d</span><a class="headerlink" href="#max-unpool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.max_unpool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">max_unpool3d</code><span class="sig-paren">(</span><em>input</em>, <em>indices</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>padding=0</em>, <em>output_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#max_unpool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.max_unpool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a partial inverse of <code class="xref py py-class docutils literal notranslate"><span class="pre">MaxPool3d</span></code>.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.MaxUnpool3d" title="torch.nn.MaxUnpool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">MaxUnpool3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="lp-pool1d">
<h3><span class="hidden-section">lp_pool1d</span><a class="headerlink" href="#lp-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.lp_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">lp_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#lp_pool1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.lp_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D power-average pooling over an input signal composed of
several input planes. If the sum of all inputs to the power of <cite>p</cite> is
zero, the gradient is set to zero as well.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.LPPool1d" title="torch.nn.LPPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">LPPool1d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="lp-pool2d">
<h3><span class="hidden-section">lp_pool2d</span><a class="headerlink" href="#lp-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.lp_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">lp_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>norm_type</em>, <em>kernel_size</em>, <em>stride=None</em>, <em>ceil_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#lp_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.lp_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D power-average pooling over an input signal composed of
several input planes. If the sum of all inputs to the power of <cite>p</cite> is
zero, the gradient is set to zero as well.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.LPPool2d" title="torch.nn.LPPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">LPPool2d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool1d">
<h3><span class="hidden-section">adaptive_max_pool1d</span><a class="headerlink" href="#adaptive-max-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool1d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.AdaptiveMaxPool1d" title="torch.nn.AdaptiveMaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool1d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_size</strong> – the target output size (single integer)</p></li>
<li><p><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool2d">
<h3><span class="hidden-section">adaptive_max_pool2d</span><a class="headerlink" href="#adaptive-max-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool2d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.AdaptiveMaxPool2d" title="torch.nn.AdaptiveMaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool2d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_size</strong> – the target output size (single integer or
double-integer tuple)</p></li>
<li><p><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="adaptive-max-pool3d">
<h3><span class="hidden-section">adaptive_max_pool3d</span><a class="headerlink" href="#adaptive-max-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_max_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_max_pool3d</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.adaptive_max_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive max pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.AdaptiveMaxPool3d" title="torch.nn.AdaptiveMaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveMaxPool3d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_size</strong> – the target output size (single integer or
triple-integer tuple)</p></li>
<li><p><strong>return_indices</strong> – whether to return pooling indices. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool1d">
<h3><span class="hidden-section">adaptive_avg_pool1d</span><a class="headerlink" href="#adaptive-avg-pool1d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool1d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool1d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 1D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.AdaptiveAvgPool1d" title="torch.nn.AdaptiveAvgPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool1d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_size</strong> – the target output size (single integer)</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool2d">
<h3><span class="hidden-section">adaptive_avg_pool2d</span><a class="headerlink" href="#adaptive-avg-pool2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool2d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_avg_pool2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 2D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.AdaptiveAvgPool2d" title="torch.nn.AdaptiveAvgPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool2d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_size</strong> – the target output size (single integer or
double-integer tuple)</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="adaptive-avg-pool3d">
<h3><span class="hidden-section">adaptive_avg_pool3d</span><a class="headerlink" href="#adaptive-avg-pool3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.adaptive_avg_pool3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">adaptive_avg_pool3d</code><span class="sig-paren">(</span><em>input</em>, <em>output_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#adaptive_avg_pool3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.adaptive_avg_pool3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a 3D adaptive average pooling over an input signal composed of
several input planes.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.AdaptiveAvgPool3d" title="torch.nn.AdaptiveAvgPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveAvgPool3d</span></code></a> for details and output shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>output_size</strong> – the target output size (single integer or
triple-integer tuple)</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="non-linear-activation-functions">
<h2>Non-linear activation functions<a class="headerlink" href="#non-linear-activation-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="threshold">
<h3><span class="hidden-section">threshold</span><a class="headerlink" href="#threshold" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.threshold">
<code class="descclassname">torch.nn.functional.</code><code class="descname">threshold</code><span class="sig-paren">(</span><em>input</em>, <em>threshold</em>, <em>value</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Thresholds each element of the input Tensor.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Threshold" title="torch.nn.Threshold"><code class="xref py py-class docutils literal notranslate"><span class="pre">Threshold</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.threshold_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">threshold_</code><span class="sig-paren">(</span><em>input</em>, <em>threshold</em>, <em>value</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.threshold_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.threshold" title="torch.nn.functional.threshold"><code class="xref py py-func docutils literal notranslate"><span class="pre">threshold()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="relu">
<h3><span class="hidden-section">relu</span><a class="headerlink" href="#relu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the rectified linear unit function element-wise. See
<a class="reference internal" href="nn.html#torch.nn.ReLU" title="torch.nn.ReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.relu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu_</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.relu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.relu" title="torch.nn.functional.relu"><code class="xref py py-func docutils literal notranslate"><span class="pre">relu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="hardtanh">
<h3><span class="hidden-section">hardtanh</span><a class="headerlink" href="#hardtanh" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hardtanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardtanh</code><span class="sig-paren">(</span><em>input</em>, <em>min_val=-1.</em>, <em>max_val=1.</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#hardtanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the HardTanh function element-wise. See <a class="reference internal" href="nn.html#torch.nn.Hardtanh" title="torch.nn.Hardtanh"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hardtanh</span></code></a> for more
details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.hardtanh_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardtanh_</code><span class="sig-paren">(</span><em>input</em>, <em>min_val=-1.</em>, <em>max_val=1.</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.hardtanh_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh"><code class="xref py py-func docutils literal notranslate"><span class="pre">hardtanh()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="relu6">
<h3><span class="hidden-section">relu6</span><a class="headerlink" href="#relu6" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.relu6">
<code class="descclassname">torch.nn.functional.</code><code class="descname">relu6</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#relu6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.relu6" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\(\text{ReLU6}(x) = \min(\max(0,x), 6)\)</span>.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.ReLU6" title="torch.nn.ReLU6"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReLU6</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="elu">
<h3><span class="hidden-section">elu</span><a class="headerlink" href="#elu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.elu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">elu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.0</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#elu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.elu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))\)</span>.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.ELU" title="torch.nn.ELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">ELU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.elu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">elu_</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.elu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.elu" title="torch.nn.functional.elu"><code class="xref py py-func docutils literal notranslate"><span class="pre">elu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="selu">
<h3><span class="hidden-section">selu</span><a class="headerlink" href="#selu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.selu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">selu</code><span class="sig-paren">(</span><em>input</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#selu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.selu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))\)</span>,
with <span class="math">\(\alpha=1.6732632423543772848170429916717\)</span> and
<span class="math">\(scale=1.0507009873554804934193349852946\)</span>.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.SELU" title="torch.nn.SELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">SELU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="celu">
<h3><span class="hidden-section">celu</span><a class="headerlink" href="#celu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.celu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">celu</code><span class="sig-paren">(</span><em>input</em>, <em>alpha=1.</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#celu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.celu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))\)</span>.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.CELU" title="torch.nn.CELU"><code class="xref py py-class docutils literal notranslate"><span class="pre">CELU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="leaky-relu">
<h3><span class="hidden-section">leaky_relu</span><a class="headerlink" href="#leaky-relu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.leaky_relu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">leaky_relu</code><span class="sig-paren">(</span><em>input</em>, <em>negative_slope=0.01</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#leaky_relu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.leaky_relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)\)</span></p>
<p>See <a class="reference internal" href="nn.html#torch.nn.LeakyReLU" title="torch.nn.LeakyReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.leaky_relu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">leaky_relu_</code><span class="sig-paren">(</span><em>input</em>, <em>negative_slope=0.01</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.leaky_relu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.leaky_relu" title="torch.nn.functional.leaky_relu"><code class="xref py py-func docutils literal notranslate"><span class="pre">leaky_relu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="prelu">
<h3><span class="hidden-section">prelu</span><a class="headerlink" href="#prelu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.prelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">prelu</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#prelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.prelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise the function
<span class="math">\(\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)\)</span> where weight is a
learnable parameter.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.PReLU" title="torch.nn.PReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">PReLU</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="rrelu">
<h3><span class="hidden-section">rrelu</span><a class="headerlink" href="#rrelu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.rrelu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">rrelu</code><span class="sig-paren">(</span><em>input</em>, <em>lower=1./8</em>, <em>upper=1./3</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#rrelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.rrelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomized leaky ReLU.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.RReLU" title="torch.nn.RReLU"><code class="xref py py-class docutils literal notranslate"><span class="pre">RReLU</span></code></a> for more details.</p>
</dd></dl>

<dl class="function">
<dt id="torch.nn.functional.rrelu_">
<code class="descclassname">torch.nn.functional.</code><code class="descname">rrelu_</code><span class="sig-paren">(</span><em>input</em>, <em>lower=1./8</em>, <em>upper=1./3</em>, <em>training=False</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.rrelu_" title="Permalink to this definition">¶</a></dt>
<dd><p>In-place version of <a class="reference internal" href="#torch.nn.functional.rrelu" title="torch.nn.functional.rrelu"><code class="xref py py-func docutils literal notranslate"><span class="pre">rrelu()</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="glu">
<h3><span class="hidden-section">glu</span><a class="headerlink" href="#glu" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.glu">
<code class="descclassname">torch.nn.functional.</code><code class="descname">glu</code><span class="sig-paren">(</span><em>input</em>, <em>dim=-1</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#glu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.glu" title="Permalink to this definition">¶</a></dt>
<dd><p>The gated linear unit. Computes:</p>
<div class="math">
\[\text{GLU}(a, b) = a \otimes \sigma(b)

\]</div>
<p>where <cite>input</cite> is split in half along <cite>dim</cite> to form <cite>a</cite> and <cite>b</cite>, <span class="math">\(\sigma\)</span>
is the sigmoid function and <span class="math">\(\otimes\)</span> is the element-wise product between matrices.</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1612.08083">Language Modeling with Gated Convolutional Networks</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input tensor</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – dimension on which to split the input. Default: -1</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="gelu">
<h3><span class="hidden-section">gelu</span><a class="headerlink" href="#gelu" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="logsigmoid">
<h3><span class="hidden-section">logsigmoid</span><a class="headerlink" href="#logsigmoid" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.logsigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">logsigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.logsigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise <span class="math">\(\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)\)</span></p>
<p>See <a class="reference internal" href="nn.html#torch.nn.LogSigmoid" title="torch.nn.LogSigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogSigmoid</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="hardshrink">
<h3><span class="hidden-section">hardshrink</span><a class="headerlink" href="#hardshrink" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hardshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hardshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#hardshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.hardshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the hard shrinkage function element-wise</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Hardshrink" title="torch.nn.Hardshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hardshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="tanhshrink">
<h3><span class="hidden-section">tanhshrink</span><a class="headerlink" href="#tanhshrink" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.tanhshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanhshrink</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#tanhshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, <span class="math">\(\text{Tanhshrink}(x) = x - \text{Tanh}(x)\)</span></p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Tanhshrink" title="torch.nn.Tanhshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tanhshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="softsign">
<h3><span class="hidden-section">softsign</span><a class="headerlink" href="#softsign" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softsign">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softsign</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#softsign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise, the function <span class="math">\(\text{SoftSign}(x) = \frac{x}{1 + |x|}\)</span></p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Softsign" title="torch.nn.Softsign"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softsign</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="softplus">
<h3><span class="hidden-section">softplus</span><a class="headerlink" href="#softplus" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softplus">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softplus</code><span class="sig-paren">(</span><em>input</em>, <em>beta=1</em>, <em>threshold=20</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.softplus" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="softmin">
<h3><span class="hidden-section">softmin</span><a class="headerlink" href="#softmin" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softmin">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmin</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#softmin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmin function.</p>
<p>Note that <span class="math">\(\text{Softmin}(x) = \text{Softmax}(-x)\)</span>. See softmax definition for mathematical formula.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Softmin" title="torch.nn.Softmin"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softmin</span></code></a> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which softmin will be computed (so every slice
along dim will sum to 1).</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="softmax">
<h3><span class="hidden-section">softmax</span><a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softmax</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmax function.</p>
<p>Softmax is defined as:</p>
<p><span class="math">\(\text{Softmax}(x_{i}) = \frac{exp(x_i)}{\sum_j exp(x_j)}\)</span></p>
<p>It is applied to all slices along dim, and will re-scale them so that the elements
lie in the range <cite>[0, 1]</cite> and sum to 1.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Softmax" title="torch.nn.Softmax"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softmax</span></code></a> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which softmax will be computed.</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function doesn’t work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use log_softmax instead (it’s faster and has better numerical properties).</p>
</div>
</dd></dl>

</div>
<div class="section" id="softshrink">
<h3><span class="hidden-section">softshrink</span><a class="headerlink" href="#softshrink" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.softshrink">
<code class="descclassname">torch.nn.functional.</code><code class="descname">softshrink</code><span class="sig-paren">(</span><em>input</em>, <em>lambd=0.5</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.softshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the soft shrinkage function elementwise</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Softshrink" title="torch.nn.Softshrink"><code class="xref py py-class docutils literal notranslate"><span class="pre">Softshrink</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="gumbel-softmax">
<h3><span class="hidden-section">gumbel_softmax</span><a class="headerlink" href="#gumbel-softmax" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.gumbel_softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">gumbel_softmax</code><span class="sig-paren">(</span><em>logits</em>, <em>tau=1</em>, <em>hard=False</em>, <em>eps=1e-10</em>, <em>dim=-1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#gumbel_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.gumbel_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples from the <a class="reference external" href="https://arxiv.org/abs/1611.00712https://arxiv.org/abs/1611.01144">Gumbel-Softmax distribution</a> and optionally discretizes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logits</strong> – <cite>[…, num_features]</cite> unnormalized log probabilities</p></li>
<li><p><strong>tau</strong> – non-negative scalar temperature</p></li>
<li><p><strong>hard</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the returned samples will be discretized as one-hot vectors,
but will be differentiated as if it is the soft sample in autograd</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which softmax will be computed. Default: -1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Sampled tensor of same shape as <cite>logits</cite> from the Gumbel-Softmax distribution.
If <code class="docutils literal notranslate"><span class="pre">hard=True</span></code>, the returned samples will be one-hot, otherwise they will
be probability distributions that sum to 1 across <cite>dim</cite>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is here for legacy reasons, may be removed from nn.Functional in the future.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The main trick for <cite>hard</cite> is to do  <cite>y_hard - y_soft.detach() + y_soft</cite></p>
<p>It achieves two things:
- makes the output value exactly one-hot
(since we add then subtract y_soft value)
- makes the gradient equal to y_soft gradient
(since we strip all other gradients)</p>
</div>
<dl>
<dt>Examples::</dt><dd><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Sample soft categorical using reparametrization trick:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Sample hard categorical using &quot;Straight-through&quot; trick:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="log-softmax">
<h3><span class="hidden-section">log_softmax</span><a class="headerlink" href="#log-softmax" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.log_softmax">
<code class="descclassname">torch.nn.functional.</code><code class="descname">log_softmax</code><span class="sig-paren">(</span><em>input</em>, <em>dim=None</em>, <em>_stacklevel=3</em>, <em>dtype=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#log_softmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.log_softmax" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a softmax followed by a logarithm.</p>
<p>While mathematically equivalent to log(softmax(x)), doing these two
operations separately is slower, and numerically unstable. This function
uses an alternative formulation to compute the output and gradient correctly.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.LogSoftmax" title="torch.nn.LogSoftmax"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogSoftmax</span></code></a> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – A dimension along which log_softmax will be computed.</p></li>
<li><p><strong>dtype</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.dtype</span></code>, optional) – the desired data type of returned tensor.
If specified, the input tensor is casted to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> before the operation
is performed. This is useful for preventing data type overflows. Default: None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="tanh">
<h3><span class="hidden-section">tanh</span><a class="headerlink" href="#tanh" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.tanh">
<code class="descclassname">torch.nn.functional.</code><code class="descname">tanh</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies element-wise,
<span class="math">\(\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}\)</span></p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Tanh" title="torch.nn.Tanh"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tanh</span></code></a> for more details.</p>
</dd></dl>

</div>
<div class="section" id="sigmoid">
<h3><span class="hidden-section">sigmoid</span><a class="headerlink" href="#sigmoid" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.sigmoid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">sigmoid</code><span class="sig-paren">(</span><em>input</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the element-wise function <span class="math">\(\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}\)</span></p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Sigmoid" title="torch.nn.Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sigmoid</span></code></a> for more details.</p>
</dd></dl>

</div>
</div>
<div class="section" id="normalization-functions">
<h2>Normalization functions<a class="headerlink" href="#normalization-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="batch-norm">
<h3><span class="hidden-section">batch_norm</span><a class="headerlink" href="#batch-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.batch_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">batch_norm</code><span class="sig-paren">(</span><em>input</em>, <em>running_mean</em>, <em>running_var</em>, <em>weight=None</em>, <em>bias=None</em>, <em>training=False</em>, <em>momentum=0.1</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#batch_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.batch_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Batch Normalization for each channel across a batch of data.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.BatchNorm1d" title="torch.nn.BatchNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm1d</span></code></a>, <a class="reference internal" href="nn.html#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm2d</span></code></a>,
<a class="reference internal" href="nn.html#torch.nn.BatchNorm3d" title="torch.nn.BatchNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="instance-norm">
<h3><span class="hidden-section">instance_norm</span><a class="headerlink" href="#instance-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.instance_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">instance_norm</code><span class="sig-paren">(</span><em>input</em>, <em>running_mean=None</em>, <em>running_var=None</em>, <em>weight=None</em>, <em>bias=None</em>, <em>use_input_stats=True</em>, <em>momentum=0.1</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#instance_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.instance_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Instance Normalization for each channel in each data sample in a
batch.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm1d</span></code></a>, <a class="reference internal" href="nn.html#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm2d</span></code></a>,
<a class="reference internal" href="nn.html#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">InstanceNorm3d</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="layer-norm">
<h3><span class="hidden-section">layer_norm</span><a class="headerlink" href="#layer-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.layer_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">layer_norm</code><span class="sig-paren">(</span><em>input</em>, <em>normalized_shape</em>, <em>weight=None</em>, <em>bias=None</em>, <em>eps=1e-05</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#layer_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.layer_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies Layer Normalization for last certain number of dimensions.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="local-response-norm">
<h3><span class="hidden-section">local_response_norm</span><a class="headerlink" href="#local-response-norm" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.local_response_norm">
<code class="descclassname">torch.nn.functional.</code><code class="descname">local_response_norm</code><span class="sig-paren">(</span><em>input</em>, <em>size</em>, <em>alpha=0.0001</em>, <em>beta=0.75</em>, <em>k=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#local_response_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.local_response_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies local response normalization over an input signal composed of
several input planes, where channels occupy the second dimension.
Applies normalization across channels.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.LocalResponseNorm" title="torch.nn.LocalResponseNorm"><code class="xref py py-class docutils literal notranslate"><span class="pre">LocalResponseNorm</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="normalize">
<h3><span class="hidden-section">normalize</span><a class="headerlink" href="#normalize" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.normalize">
<code class="descclassname">torch.nn.functional.</code><code class="descname">normalize</code><span class="sig-paren">(</span><em>input</em>, <em>p=2</em>, <em>dim=1</em>, <em>eps=1e-12</em>, <em>out=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#normalize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs <span class="math">\(L_p\)</span> normalization of inputs over specified dimension.</p>
<p>For a tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> of sizes <span class="math">\((n_0, ..., n_{dim}, ..., n_k)\)</span>, each
<span class="math">\(n_{dim}\)</span> -element vector <span class="math">\(v\)</span> along dimension <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> is transformed as</p>
<div class="math">
\[v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}.

\]</div>
<p>With the default arguments it uses the Euclidean norm over vectors along dimension <span class="math">\(1\)</span> for normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of any shape</p></li>
<li><p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – the exponent value in the norm formulation. Default: 2</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the dimension to reduce. Default: 1</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a>) – small value to avoid division by zero. Default: 1e-12</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – the output tensor. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is used, this
operation won’t be differentiable.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="linear-functions">
<h2>Linear functions<a class="headerlink" href="#linear-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="linear">
<h3><span class="hidden-section">linear</span><a class="headerlink" href="#linear" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.linear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">linear</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math">\(y = xA^T + b\)</span>.</p>
<p>Shape:</p>
<blockquote>
<div><ul class="simple">
<li><p>Input: <span class="math">\((N, *, in\_features)\)</span> where <cite>*</cite> means any number of
additional dimensions</p></li>
<li><p>Weight: <span class="math">\((out\_features, in\_features)\)</span></p></li>
<li><p>Bias: <span class="math">\((out\_features)\)</span></p></li>
<li><p>Output: <span class="math">\((N, *, out\_features)\)</span></p></li>
</ul>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="bilinear">
<h3><span class="hidden-section">bilinear</span><a class="headerlink" href="#bilinear" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.bilinear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">bilinear</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>weight</em>, <em>bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.bilinear" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>
<div class="section" id="dropout-functions">
<h2>Dropout functions<a class="headerlink" href="#dropout-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dropout">
<h3><span class="hidden-section">dropout</span><a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=True</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>During training, randomly zeroes some of the elements of the input
tensor with probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli
distribution.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Dropout" title="torch.nn.Dropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – probability of an element to be zeroed. Default: 0.5</p></li>
<li><p><strong>training</strong> – apply dropout if is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="alpha-dropout">
<h3><span class="hidden-section">alpha_dropout</span><a class="headerlink" href="#alpha-dropout" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.alpha_dropout">
<code class="descclassname">torch.nn.functional.</code><code class="descname">alpha_dropout</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=False</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#alpha_dropout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.alpha_dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies alpha dropout to the input.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.AlphaDropout" title="torch.nn.AlphaDropout"><code class="xref py py-class docutils literal notranslate"><span class="pre">AlphaDropout</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="dropout2d">
<h3><span class="hidden-section">dropout2d</span><a class="headerlink" href="#dropout2d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout2d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout2d</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=True</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zero out entire channels (a channel is a 2D feature map,
e.g., the <span class="math">\(j\)</span>-th channel of the <span class="math">\(i\)</span>-th sample in the
batched input is a 2D tensor <span class="math">\(\text{input}[i, j]\)</span>) of the input tensor).
Each channel will be zeroed out independently on every forward call with
probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Dropout2d" title="torch.nn.Dropout2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout2d</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – probability of a channel to be zeroed. Default: 0.5</p></li>
<li><p><strong>training</strong> – apply dropout if is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="dropout3d">
<h3><span class="hidden-section">dropout3d</span><a class="headerlink" href="#dropout3d" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.dropout3d">
<code class="descclassname">torch.nn.functional.</code><code class="descname">dropout3d</code><span class="sig-paren">(</span><em>input</em>, <em>p=0.5</em>, <em>training=True</em>, <em>inplace=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#dropout3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.dropout3d" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly zero out entire channels (a channel is a 3D feature map,
e.g., the <span class="math">\(j\)</span>-th channel of the <span class="math">\(i\)</span>-th sample in the
batched input is a 3D tensor <span class="math">\(\text{input}[i, j]\)</span>) of the input tensor).
Each channel will be zeroed out independently on every forward call with
probability <code class="xref py py-attr docutils literal notranslate"><span class="pre">p</span></code> using samples from a Bernoulli distribution.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Dropout3d" title="torch.nn.Dropout3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout3d</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>p</strong> – probability of a channel to be zeroed. Default: 0.5</p></li>
<li><p><strong>training</strong> – apply dropout if is <code class="docutils literal notranslate"><span class="pre">True</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>inplace</strong> – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, will do this operation in-place. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="sparse-functions">
<h2>Sparse functions<a class="headerlink" href="#sparse-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="embedding">
<h3><span class="hidden-section">embedding</span><a class="headerlink" href="#embedding" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.embedding">
<code class="descclassname">torch.nn.functional.</code><code class="descname">embedding</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>padding_idx=None</em>, <em>max_norm=None</em>, <em>norm_type=2.0</em>, <em>scale_grad_by_freq=False</em>, <em>sparse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple lookup table that looks up embeddings in a fixed dictionary and size.</p>
<p>This module is often used to retrieve word embeddings using indices.
The input to the module is a list of indices, and the embedding matrix,
and the output is the corresponding word embeddings.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LongTensor</em>) – Tensor containing indices into the embedding matrix</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The embedding matrix with number of rows equal to the maximum possible index + 1,
and number of columns equal to the embedding size</p></li>
<li><p><strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – If given, pads the output with the embedding vector at <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_idx</span></code>
(initialized to zeros) whenever it encounters the index.</p></li>
<li><p><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>
is renormalized to have norm <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>.
Note: this will modify <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> in-place.</p></li>
<li><p><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code> option. Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> will be a sparse tensor. See Notes under
<a class="reference internal" href="nn.html#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details regarding sparse gradients.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: LongTensor of arbitrary shape containing the indices to extract</p></li>
<li><dl class="simple">
<dt>Weight: Embedding matrix of floating point type with shape <cite>(V, embedding_dim)</cite>,</dt><dd><p>where V = maximum index + 1 and embedding_dim = the embedding size</p>
</dd>
</dl>
</li>
<li><p>Output: <cite>(*, embedding_dim)</cite>, where <cite>*</cite> is the input shape</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># an embedding matrix containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">)</span>
<span class="go">tensor([[[ 0.8490,  0.9625,  0.6753],</span>
<span class="go">         [ 0.9666,  0.7761,  0.6108],</span>
<span class="go">         [ 0.6246,  0.9751,  0.3618],</span>
<span class="go">         [ 0.4161,  0.2419,  0.7383]],</span>

<span class="go">        [[ 0.6246,  0.9751,  0.3618],</span>
<span class="go">         [ 0.0237,  0.7794,  0.0528],</span>
<span class="go">         [ 0.9666,  0.7761,  0.6108],</span>
<span class="go">         [ 0.3385,  0.8612,  0.1867]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># example with padding_idx</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">tensor([[[ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.5609,  0.5384,  0.8720],</span>
<span class="go">         [ 0.0000,  0.0000,  0.0000],</span>
<span class="go">         [ 0.6262,  0.2438,  0.7471]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="embedding-bag">
<h3><span class="hidden-section">embedding_bag</span><a class="headerlink" href="#embedding-bag" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.embedding_bag">
<code class="descclassname">torch.nn.functional.</code><code class="descname">embedding_bag</code><span class="sig-paren">(</span><em>input</em>, <em>weight</em>, <em>offsets=None</em>, <em>max_norm=None</em>, <em>norm_type=2</em>, <em>scale_grad_by_freq=False</em>, <em>mode='mean'</em>, <em>sparse=False</em>, <em>per_sample_weights=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#embedding_bag"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.embedding_bag" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes sums, means or maxes of <cite>bags</cite> of embeddings, without instantiating the
intermediate embeddings.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.EmbeddingBag</span></code></a> for more details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in its backward pass that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>LongTensor</em>) – Tensor containing bags of indices into the embedding matrix</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – The embedding matrix with number of rows equal to the maximum possible index + 1,
and number of columns equal to the embedding size</p></li>
<li><p><strong>offsets</strong> (<em>LongTensor</em><em>, </em><em>optional</em>) – Only used when <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 1D. <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> determines
the starting index position of each bag (sequence) in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>
is renormalized to have norm <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code>.
Note: this will modify <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> in-place.</p></li>
<li><p><strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – The <code class="docutils literal notranslate"><span class="pre">p</span></code> in the <code class="docutils literal notranslate"><span class="pre">p</span></code>-norm to compute for the <code class="xref py py-attr docutils literal notranslate"><span class="pre">max_norm</span></code> option.
Default <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p></li>
<li><p><strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – if given, this will scale gradients by the inverse of frequency of
the words in the mini-batch. Default <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Note: this option is not supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</p></li>
<li><p><strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code class="docutils literal notranslate"><span class="pre">&quot;sum&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code>. Specifies the way to reduce the bag.
Default: <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code></p></li>
<li><p><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradient w.r.t. <code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> will be a sparse tensor. See Notes under
<a class="reference internal" href="nn.html#torch.nn.Embedding" title="torch.nn.Embedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Embedding</span></code></a> for more details regarding sparse gradients.
Note: this option is not supported when <code class="docutils literal notranslate"><span class="pre">mode=&quot;max&quot;</span></code>.</p></li>
<li><p><strong>per_sample_weights</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a tensor of float / double weights, or None
to indicate all weights should be taken to be 1. If specified, <code class="xref py py-attr docutils literal notranslate"><span class="pre">per_sample_weights</span></code>
must have exactly the same shape as input and is treated as having the same
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code>, if those are not None.</p></li>
</ul>
</dd>
</dl>
<p>Shape:</p>
<blockquote>
<div><ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> (LongTensor) and <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> (LongTensor, optional)</p>
<ul>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 2D of shape <cite>(B, N)</cite>,</p>
<p>it will be treated as <code class="docutils literal notranslate"><span class="pre">B</span></code> bags (sequences) each of fixed length <code class="docutils literal notranslate"><span class="pre">N</span></code>, and
this will return <code class="docutils literal notranslate"><span class="pre">B</span></code> values aggregated in a way depending on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is ignored and required to be <code class="docutils literal notranslate"><span class="pre">None</span></code> in this case.</p>
</li>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is 1D of shape <cite>(N)</cite>,</p>
<p>it will be treated as a concatenation of multiple bags (sequences).
<code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> is required to be a 1D tensor containing the
starting index positions of each bag in <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>. Therefore,
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">offsets</span></code> of shape <cite>(B)</cite>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> will be viewed as
having <code class="docutils literal notranslate"><span class="pre">B</span></code> bags. Empty bags (i.e., having 0-length) will have
returned vectors filled by zeros.</p>
</li>
</ul>
</li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code> (Tensor): the learnable weights of the module of
shape <cite>(num_embeddings, embedding_dim)</cite></p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">per_sample_weights</span></code> (Tensor, optional). Has the same shape as
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output</span></code>: aggregated embedding values of shape <cite>(B, embedding_dim)</cite></p></li>
</ul>
</div></blockquote>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># an Embedding module containing 10 tensors of size 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">embedding_bag</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>
<span class="go">tensor([[ 0.3397,  0.3552,  0.5545],</span>
<span class="go">        [ 0.5893,  0.4386,  0.5882]])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="one-hot">
<h3><span class="hidden-section">one_hot</span><a class="headerlink" href="#one-hot" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.one_hot">
<code class="descclassname">torch.nn.functional.</code><code class="descname">one_hot</code><span class="sig-paren">(</span><em>tensor</em>, <em>num_classes=0</em><span class="sig-paren">)</span> &#x2192; LongTensor<a class="headerlink" href="#torch.nn.functional.one_hot" title="Permalink to this definition">¶</a></dt>
<dd><p>Takes LongTensor with index values of shape <code class="docutils literal notranslate"><span class="pre">(*)</span></code> and returns a tensor
of shape <code class="docutils literal notranslate"><span class="pre">(*,</span> <span class="pre">num_classes)</span></code> that have zeros everywhere except where the
index of last dimension matches the corresponding value of the input tensor,
in which case it will be 1.</p>
<p>See also <a class="reference external" href="https://en.wikipedia.org/wiki/One-hot">One-hot on Wikipedia</a> .</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<em>LongTensor</em>) – class values of any shape.</p></li>
<li><p><strong>num_classes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – Total number of classes. If set to -1, the number
of classes will be inferred as one greater than the largest class
value in the input tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>LongTensor that has one more dimension with 1 values at the
index of last dimension indicated by the input, and 0 everywhere
else.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[1, 0, 0],</span>
<span class="go">        [0, 1, 0],</span>
<span class="go">        [0, 0, 1],</span>
<span class="go">        [1, 0, 0],</span>
<span class="go">        [0, 1, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">tensor([[1, 0, 0, 0, 0],</span>
<span class="go">        [0, 1, 0, 0, 0],</span>
<span class="go">        [0, 0, 1, 0, 0],</span>
<span class="go">        [1, 0, 0, 0, 0],</span>
<span class="go">        [0, 1, 0, 0, 0]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[[1, 0, 0],</span>
<span class="go">         [0, 1, 0]],</span>
<span class="go">        [[0, 0, 1],</span>
<span class="go">         [1, 0, 0]],</span>
<span class="go">        [[0, 1, 0],</span>
<span class="go">         [0, 0, 1]]])</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="distance-functions">
<h2>Distance functions<a class="headerlink" href="#distance-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pairwise-distance">
<h3><span class="hidden-section">pairwise_distance</span><a class="headerlink" href="#pairwise-distance" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pairwise_distance">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pairwise_distance</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em>, <em>p=2.0</em>, <em>eps=1e-06</em>, <em>keepdim=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#pairwise_distance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.pairwise_distance" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="nn.html#torch.nn.PairwiseDistance" title="torch.nn.PairwiseDistance"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.PairwiseDistance</span></code></a> for details</p>
</dd></dl>

</div>
<div class="section" id="cosine-similarity">
<h3><span class="hidden-section">cosine_similarity</span><a class="headerlink" href="#cosine-similarity" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cosine_similarity">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cosine_similarity</code><span class="sig-paren">(</span><em>x1</em>, <em>x2</em>, <em>dim=1</em>, <em>eps=1e-8</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.cosine_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cosine similarity between x1 and x2, computed along dim.</p>
<div class="math">
\[\text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}

\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x1</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – First input.</p></li>
<li><p><strong>x2</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – Second input (of size matching x1).</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Dimension of vectors. Default: 1</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid division by zero.
Default: 1e-8</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math">\((\ast_1, D, \ast_2)\)</span> where D is at position <cite>dim</cite>.</p></li>
<li><p>Output: <span class="math">\((\ast_1, \ast_2)\)</span> where 1 is at position <cite>dim</cite>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pdist">
<h3><span class="hidden-section">pdist</span><a class="headerlink" href="#pdist" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pdist">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pdist</code><span class="sig-paren">(</span><em>input</em>, <em>p=2</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="headerlink" href="#torch.nn.functional.pdist" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the p-norm distance between every pair of row vectors in the input.
This is identical to the upper triangular portion, excluding the diagonal, of
<cite>torch.norm(input[:, None] - input, dim=2, p=p)</cite>. This function will be faster
if the rows are contiguous.</p>
<p>If input has shape <span class="math">\(N \times M\)</span> then the output will have shape
<span class="math">\(\frac{1}{2} N (N - 1)\)</span>.</p>
<p>This function is equivalent to <cite>scipy.spatial.distance.pdist(input,
‘minkowski’, p=p)</cite> if <span class="math">\(p \in (0, \infty)\)</span>. When <span class="math">\(p = 0\)</span> it is
equivalent to <cite>scipy.spatial.distance.pdist(input, ‘hamming’) * M</cite>.
When <span class="math">\(p = \infty\)</span>, the closest scipy function is
<cite>scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – input tensor of shape <span class="math">\(N \times M\)</span>.</p></li>
<li><p><strong>p</strong> – p value for the p-norm distance to calculate between each vector pair
<span class="math">\(\in [0, \infty]\)</span>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="binary-cross-entropy">
<h3><span class="hidden-section">binary_cross_entropy</span><a class="headerlink" href="#binary-cross-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.binary_cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">binary_cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#binary_cross_entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.binary_cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that measures the Binary Cross Entropy
between the target and the output.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.BCELoss" title="torch.nn.BCELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BCELoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – Tensor of arbitrary shape</p></li>
<li><p><strong>target</strong> – Tensor of the same shape as input</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight
if provided it’s repeated to match input tensor shape</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="binary-cross-entropy-with-logits">
<h3><span class="hidden-section">binary_cross_entropy_with_logits</span><a class="headerlink" href="#binary-cross-entropy-with-logits" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.binary_cross_entropy_with_logits">
<code class="descclassname">torch.nn.functional.</code><code class="descname">binary_cross_entropy_with_logits</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em>, <em>pos_weight=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#binary_cross_entropy_with_logits"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.binary_cross_entropy_with_logits" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that measures Binary Cross Entropy between target and output
logits.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.BCEWithLogitsLoss" title="torch.nn.BCEWithLogitsLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BCEWithLogitsLoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – Tensor of arbitrary shape</p></li>
<li><p><strong>target</strong> – Tensor of the same shape as input</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight
if provided it’s repeated to match input tensor shape</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
<li><p><strong>pos_weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a weight of positive examples.
Must be a vector with length equal to the number of classes.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="poisson-nll-loss">
<h3><span class="hidden-section">poisson_nll_loss</span><a class="headerlink" href="#poisson-nll-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.poisson_nll_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">poisson_nll_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>log_input=True</em>, <em>full=False</em>, <em>size_average=None</em>, <em>eps=1e-08</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#poisson_nll_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.poisson_nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Poisson negative log likelihood loss.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.PoissonNLLLoss" title="torch.nn.PoissonNLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">PoissonNLLLoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – expectation of underlying Poisson distribution.</p></li>
<li><p><strong>target</strong> – random sample <span class="math">\(target \sim \text{Poisson}(input)\)</span>.</p></li>
<li><p><strong>log_input</strong> – if <code class="docutils literal notranslate"><span class="pre">True</span></code> the loss is computed as
<span class="math">\(\exp(\text{input}) - \text{target} * \text{input}\)</span>, if <code class="docutils literal notranslate"><span class="pre">False</span></code> then loss is
<span class="math">\(\text{input} - \text{target} * \log(\text{input}+\text{eps})\)</span>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>full</strong> – whether to compute full loss, i. e. to add the Stirling
approximation term. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>
<span class="math">\(\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})\)</span>.</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>, </em><em>optional</em>) – Small value to avoid evaluation of <span class="math">\(\log(0)\)</span> when
<code class="xref py py-attr docutils literal notranslate"><span class="pre">log_input`=``False`</span></code>. Default: 1e-8</p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="cosine-embedding-loss">
<h3><span class="hidden-section">cosine_embedding_loss</span><a class="headerlink" href="#cosine-embedding-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cosine_embedding_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cosine_embedding_loss</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>target</em>, <em>margin=0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#cosine_embedding_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.cosine_embedding_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="nn.html#torch.nn.CosineEmbeddingLoss" title="torch.nn.CosineEmbeddingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CosineEmbeddingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="cross-entropy">
<h3><span class="hidden-section">cross_entropy</span><a class="headerlink" href="#cross-entropy" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.cross_entropy">
<code class="descclassname">torch.nn.functional.</code><code class="descname">cross_entropy</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#cross_entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>This criterion combines <cite>log_softmax</cite> and <cite>nll_loss</cite> in a single
function.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.CrossEntropyLoss" title="torch.nn.CrossEntropyLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite> or <span class="math">\((N, C, H, W)\)</span>
in case of 2D Loss, or <span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K \geq 1\)</span>
in the case of K-dimensional loss.</p></li>
<li><p><strong>target</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – <span class="math">\((N)\)</span> where each value is <span class="math">\(0 \leq \text{targets}[i] \leq C-1\)</span>,
or <span class="math">\((N, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K \geq 1\)</span> for
K-dimensional loss.</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, has to be a Tensor of size <cite>C</cite></p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets. Default: -100</p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="ctc-loss">
<h3><span class="hidden-section">ctc_loss</span><a class="headerlink" href="#ctc-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.ctc_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">ctc_loss</code><span class="sig-paren">(</span><em>log_probs</em>, <em>targets</em>, <em>input_lengths</em>, <em>target_lengths</em>, <em>blank=0</em>, <em>reduction='mean'</em>, <em>zero_infinity=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#ctc_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.ctc_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The Connectionist Temporal Classification loss.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.CTCLoss" title="torch.nn.CTCLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CTCLoss</span></code></a> for details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code class="docutils literal notranslate"><span class="pre">torch.backends.cudnn.deterministic</span> <span class="pre">=</span>
<span class="pre">True</span></code>.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in its backward pass that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_probs</strong> – <span class="math">\((T, N, C)\)</span> where <cite>C = number of characters in alphabet including blank</cite>,
<cite>T = input length</cite>, and <cite>N = batch size</cite>.
The logarithmized probabilities of the outputs
(e.g. obtained with <a class="reference internal" href="#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.log_softmax()</span></code></a>).</p></li>
<li><p><strong>targets</strong> – <span class="math">\((N, S)\)</span> or <cite>(sum(target_lengths))</cite>.
Targets cannot be blank. In the second form, the targets are assumed to be concatenated.</p></li>
<li><p><strong>input_lengths</strong> – <span class="math">\((N)\)</span>.
Lengths of the inputs (must each be <span class="math">\(\leq T\)</span>)</p></li>
<li><p><strong>target_lengths</strong> – <span class="math">\((N)\)</span>.
Lengths of the targets</p></li>
<li><p><strong>blank</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Blank label. Default <span class="math">\(0\)</span>.</p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the output losses will be divided by the target lengths and
then the mean over the batch is taken, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be
summed. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
<li><p><strong>zero_infinity</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to zero infinite losses and the associated gradients.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>
Infinite losses mainly occur when the inputs are too short
to be aligned to the targets.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">16</span><span class="p">,),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">30</span><span class="p">,(</span><span class="mi">16</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="hinge-embedding-loss">
<h3><span class="hidden-section">hinge_embedding_loss</span><a class="headerlink" href="#hinge-embedding-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.hinge_embedding_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">hinge_embedding_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>margin=1.0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#hinge_embedding_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.hinge_embedding_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="nn.html#torch.nn.HingeEmbeddingLoss" title="torch.nn.HingeEmbeddingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">HingeEmbeddingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="kl-div">
<h3><span class="hidden-section">kl_div</span><a class="headerlink" href="#kl-div" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.kl_div">
<code class="descclassname">torch.nn.functional.</code><code class="descname">kl_div</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#kl_div"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.kl_div" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a href="#id1"><span class="problematic" id="id2">`Kullback-Leibler divergence`_</span></a> Loss.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.KLDivLoss" title="torch.nn.KLDivLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">KLDivLoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – Tensor of arbitrary shape</p></li>
<li><p><strong>target</strong> – Tensor of the same shape as input</p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code>.
<code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied
<code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code>: the sum of the output will be divided by the batchsize
<code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the output will be divided by the number of elements in the output
Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated,
and in the meantime, specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>:attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> doesn’t return the true kl divergence value, please use
:attr:<code class="docutils literal notranslate"><span class="pre">reduction</span></code> = <code class="docutils literal notranslate"><span class="pre">'batchmean'</span></code> which aligns with KL math definition.
In the next major release, <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> will be changed to be the same as ‘batchmean’.</p>
</div>
</dd></dl>

</div>
<div class="section" id="l1-loss">
<h3><span class="hidden-section">l1_loss</span><a class="headerlink" href="#l1-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.l1_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">l1_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#l1_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.l1_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that takes the mean element-wise absolute value difference.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.L1Loss" title="torch.nn.L1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">L1Loss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="mse-loss">
<h3><span class="hidden-section">mse_loss</span><a class="headerlink" href="#mse-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.mse_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">mse_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#mse_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.mse_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Measures the element-wise mean squared error.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.MSELoss" title="torch.nn.MSELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MSELoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="margin-ranking-loss">
<h3><span class="hidden-section">margin_ranking_loss</span><a class="headerlink" href="#margin-ranking-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.margin_ranking_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">margin_ranking_loss</code><span class="sig-paren">(</span><em>input1</em>, <em>input2</em>, <em>target</em>, <em>margin=0</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#margin_ranking_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.margin_ranking_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="nn.html#torch.nn.MarginRankingLoss" title="torch.nn.MarginRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarginRankingLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multilabel-margin-loss">
<h3><span class="hidden-section">multilabel_margin_loss</span><a class="headerlink" href="#multilabel-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multilabel_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multilabel_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#multilabel_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.multilabel_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="nn.html#torch.nn.MultiLabelMarginLoss" title="torch.nn.MultiLabelMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiLabelMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multilabel-soft-margin-loss">
<h3><span class="hidden-section">multilabel_soft_margin_loss</span><a class="headerlink" href="#multilabel-soft-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multilabel_soft_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multilabel_soft_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#multilabel_soft_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.multilabel_soft_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="nn.html#torch.nn.MultiLabelSoftMarginLoss" title="torch.nn.MultiLabelSoftMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiLabelSoftMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="multi-margin-loss">
<h3><span class="hidden-section">multi_margin_loss</span><a class="headerlink" href="#multi-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.multi_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">multi_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>p=1</em>, <em>margin=1.0</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#multi_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.multi_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,</dt><dd><p>reduce=None, reduction=’mean’) -&gt; Tensor</p>
</dd>
</dl>
<p>See <a class="reference internal" href="nn.html#torch.nn.MultiMarginLoss" title="torch.nn.MultiMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="nll-loss">
<h3><span class="hidden-section">nll_loss</span><a class="headerlink" href="#nll-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.nll_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">nll_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>weight=None</em>, <em>size_average=None</em>, <em>ignore_index=-100</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#nll_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.nll_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The negative log likelihood loss.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">NLLLoss</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> – <span class="math">\((N, C)\)</span> where <cite>C = number of classes</cite> or <span class="math">\((N, C, H, W)\)</span>
in case of 2D Loss, or <span class="math">\((N, C, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K \geq 1\)</span>
in the case of K-dimensional loss.</p></li>
<li><p><strong>target</strong> – <span class="math">\((N)\)</span> where each value is <span class="math">\(0 \leq \text{targets}[i] \leq C-1\)</span>,
or <span class="math">\((N, d_1, d_2, ..., d_K)\)</span> where <span class="math">\(K \geq 1\)</span> for
K-dimensional loss.</p></li>
<li><p><strong>weight</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em>) – a manual rescaling weight given to each
class. If given, has to be a Tensor of size <cite>C</cite></p></li>
<li><p><strong>size_average</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default,
the losses are averaged over each loss element in the batch. Note that for
some losses, there multiple elements per sample. If the field <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the losses are instead summed for each minibatch. Ignored
when reduce is <code class="docutils literal notranslate"><span class="pre">False</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>ignore_index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><em>optional</em>) – Specifies a target value that is ignored
and does not contribute to the input gradient. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code> is
<code class="docutils literal notranslate"><span class="pre">True</span></code>, the loss is averaged over non-ignored targets. Default: -100</p></li>
<li><p><strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Deprecated (see <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>). By default, the
losses are averaged or summed over observations for each minibatch depending
on <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. When <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, returns a loss per
batch element instead and ignores <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'mean'</span></code> | <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>. <code class="docutils literal notranslate"><span class="pre">'none'</span></code>: no reduction will be applied,
<code class="docutils literal notranslate"><span class="pre">'mean'</span></code>: the sum of the output will be divided by the number of
elements in the output, <code class="docutils literal notranslate"><span class="pre">'sum'</span></code>: the output will be summed. Note: <code class="xref py py-attr docutils literal notranslate"><span class="pre">size_average</span></code>
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduce</span></code> are in the process of being deprecated, and in the meantime,
specifying either of those two args will override <code class="xref py py-attr docutils literal notranslate"><span class="pre">reduction</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'mean'</span></code></p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># input is of size N x C = 3 x 5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># each element in target has to have 0 &lt;= value &lt; C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="smooth-l1-loss">
<h3><span class="hidden-section">smooth_l1_loss</span><a class="headerlink" href="#smooth-l1-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.smooth_l1_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">smooth_l1_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#smooth_l1_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.smooth_l1_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that uses a squared term if the absolute
element-wise error falls below 1 and an L1 term otherwise.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.SmoothL1Loss" title="torch.nn.SmoothL1Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">SmoothL1Loss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="soft-margin-loss">
<h3><span class="hidden-section">soft_margin_loss</span><a class="headerlink" href="#soft-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.soft_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">soft_margin_loss</code><span class="sig-paren">(</span><em>input</em>, <em>target</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span> &#x2192; Tensor<a class="reference internal" href="_modules/torch/nn/functional.html#soft_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.soft_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="nn.html#torch.nn.SoftMarginLoss" title="torch.nn.SoftMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">SoftMarginLoss</span></code></a> for details.</p>
</dd></dl>

</div>
<div class="section" id="triplet-margin-loss">
<h3><span class="hidden-section">triplet_margin_loss</span><a class="headerlink" href="#triplet-margin-loss" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.triplet_margin_loss">
<code class="descclassname">torch.nn.functional.</code><code class="descname">triplet_margin_loss</code><span class="sig-paren">(</span><em>anchor</em>, <em>positive</em>, <em>negative</em>, <em>margin=1.0</em>, <em>p=2</em>, <em>eps=1e-06</em>, <em>swap=False</em>, <em>size_average=None</em>, <em>reduce=None</em>, <em>reduction='mean'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#triplet_margin_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.triplet_margin_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="nn.html#torch.nn.TripletMarginLoss" title="torch.nn.TripletMarginLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">TripletMarginLoss</span></code></a> for details</p>
</dd></dl>

</div>
</div>
<div class="section" id="vision-functions">
<h2>Vision functions<a class="headerlink" href="#vision-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pixel-shuffle">
<h3><span class="hidden-section">pixel_shuffle</span><a class="headerlink" href="#pixel-shuffle" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pixel_shuffle">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pixel_shuffle</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.nn.functional.pixel_shuffle" title="Permalink to this definition">¶</a></dt>
<dd><p>Rearranges elements in a tensor of shape <span class="math">\((*, C \times r^2, H, W)\)</span> to a
tensor of shape <span class="math">\((*, C, H \times r, W \times r)\)</span>.</p>
<p>See <a class="reference internal" href="nn.html#torch.nn.PixelShuffle" title="torch.nn.PixelShuffle"><code class="xref py py-class docutils literal notranslate"><span class="pre">PixelShuffle</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>upscale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – factor to increase spatial resolution by</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pixel_shuffle</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([1, 1, 12, 12])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pad">
<h3><span class="hidden-section">pad</span><a class="headerlink" href="#pad" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.pad">
<code class="descclassname">torch.nn.functional.</code><code class="descname">pad</code><span class="sig-paren">(</span><em>input</em>, <em>pad</em>, <em>mode='constant'</em>, <em>value=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#pad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.pad" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads tensor.</p>
<dl class="simple">
<dt>Padding size:</dt><dd><p>The padding size by which to pad some dimensions of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>
are described starting from the last dimension and moving forward.
<span class="math">\(\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor\)</span> dimensions
of <code class="docutils literal notranslate"><span class="pre">input</span></code> will be padded.
For example, to pad only the last dimension of the input tensor, then
<a class="reference internal" href="#torch.nn.functional.pad" title="torch.nn.functional.pad"><code class="xref py py-attr docutils literal notranslate"><span class="pre">pad</span></code></a> has the form
<span class="math">\((\text{padding\_left}, \text{padding\_right})\)</span>;
to pad the last 2 dimensions of the input tensor, then use
<span class="math">\((\text{padding\_left}, \text{padding\_right},\)</span>
<span class="math">\(\text{padding\_top}, \text{padding\_bottom})\)</span>;
to pad the last 3 dimensions, use
<span class="math">\((\text{padding\_left}, \text{padding\_right},\)</span>
<span class="math">\(\text{padding\_top}, \text{padding\_bottom}\)</span>
<span class="math">\(\text{padding\_front}, \text{padding\_back})\)</span>.</p>
</dd>
<dt>Padding mode:</dt><dd><p>See <a class="reference internal" href="nn.html#torch.nn.ConstantPad2d" title="torch.nn.ConstantPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ConstantPad2d</span></code></a>, <a class="reference internal" href="nn.html#torch.nn.ReflectionPad2d" title="torch.nn.ReflectionPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ReflectionPad2d</span></code></a>, and
<a class="reference internal" href="nn.html#torch.nn.ReplicationPad2d" title="torch.nn.ReplicationPad2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ReplicationPad2d</span></code></a> for concrete examples on how each of the
padding modes works. Constant padding is implemented for arbitrary dimensions.
Replicate padding is implemented for padding the last 3 dimensions of 5D input
tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of
3D input tensor. Reflect padding is only implemented for padding the last 2
dimensions of 4D input tensor, or the last dimension of 3D input tensor.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in its backward pass that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – N-dimensional tensor</p></li>
<li><p><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – m-elements tuple, where
<span class="math">\(\frac{m}{2} \leq\)</span> input dimensions and <span class="math">\(m\)</span> is even.</p></li>
<li><p><strong>mode</strong> – <code class="docutils literal notranslate"><span class="pre">'constant'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>, <code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">'constant'</span></code></p></li>
<li><p><strong>value</strong> – fill value for <code class="docutils literal notranslate"><span class="pre">'constant'</span></code> padding. Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">t4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p1d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># pad last dim by 1 on each side</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p1d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># effectively zero padding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 3, 4, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p2d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># pad last dim by (1, 1) and 2nd to last by (2, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p2d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 3, 8, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t4d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p3d</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># pad by (0, 1), (2, 1), and (3, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">t4d</span><span class="p">,</span> <span class="n">p3d</span><span class="p">,</span> <span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">torch.Size([3, 9, 7, 3])</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="interpolate">
<h3><span class="hidden-section">interpolate</span><a class="headerlink" href="#interpolate" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.interpolate">
<code class="descclassname">torch.nn.functional.</code><code class="descname">interpolate</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#interpolate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.interpolate" title="Permalink to this definition">¶</a></dt>
<dd><p>Down/up samples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p>
<p>The algorithm used for interpolation is determined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.</p>
<p>Currently temporal, spatial and volumetric sampling are supported, i.e.
expected inputs are 3-D, 4-D or 5-D in shape.</p>
<p>The input dimensions are interpreted in the form:
<cite>mini-batch x channels x [optional depth] x [optional height] x width</cite>.</p>
<p>The modes available for resizing are: <cite>nearest</cite>, <cite>linear</cite> (3D-only),
<cite>bilinear</cite>, <cite>bicubic</cite> (4D-only), <cite>trilinear</cite> (5D-only), <cite>area</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em>) – multiplier for spatial size. Has to match input size if it is a tuple.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – algorithm used for upsampling:
<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code> | <code class="docutils literal notranslate"><span class="pre">'linear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code> |
<code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'area'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code></p></li>
<li><p><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Geometrically, we consider the pixels of the
input and output as squares rather than points.
If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the input and output tensors are aligned by the
center points of their corner pixels. If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the input and
output tensors are aligned by the corner points of their corner
pixels, and the interpolation uses edge value padding for out-of-boundary values.
This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <code class="docutils literal notranslate"><span class="pre">'linear'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference internal" href="nn.html#torch.nn.Upsample" title="torch.nn.Upsample"><code class="xref py py-class docutils literal notranslate"><span class="pre">Upsample</span></code></a> for concrete examples on how this
affects the outputs.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in its backward pass that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
</dd></dl>

</div>
<div class="section" id="upsample">
<h3><span class="hidden-section">upsample</span><a class="headerlink" href="#upsample" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em>, <em>mode='nearest'</em>, <em>align_corners=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input to either the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">size</span></code> or the given
<code class="xref py py-attr docutils literal notranslate"><span class="pre">scale_factor</span></code></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...)</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in its backward pass that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<p>The algorithm used for upsampling is determined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code>.</p>
<p>Currently temporal, spatial and volumetric upsampling are supported, i.e.
expected inputs are 3-D, 4-D or 5-D in shape.</p>
<p>The input dimensions are interpreted in the form:
<cite>mini-batch x channels x [optional depth] x [optional height] x width</cite>.</p>
<p>The modes available for upsampling are: <cite>nearest</cite>, <cite>linear</cite> (3D-only),
<cite>bilinear</cite>, <cite>bicubic</cite> (4D-only), <cite>trilinear</cite> (5D-only)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – the input tensor</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.7)"><em>float</em></a><em>]</em>) – multiplier for spatial size. Has to be an integer.</p></li>
<li><p><strong>mode</strong> (<em>string</em>) – algorithm used for upsampling:
<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code> | <code class="docutils literal notranslate"><span class="pre">'linear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code> |
<code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code></p></li>
<li><p><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a><em>, </em><em>optional</em>) – Geometrically, we consider the pixels of the
input and output as squares rather than points.
If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the input and output tensors are aligned by the
center points of their corner pixels. If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the input and
output tensors are aligned by the corner points of their corner
pixels, and the interpolation uses edge value padding for out-of-boundary values.
This only has effect when <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> is <code class="docutils literal notranslate"><span class="pre">'linear'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code>, <code class="docutils literal notranslate"><span class="pre">'bicubic'</span></code> or <code class="docutils literal notranslate"><span class="pre">'trilinear'</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>With <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">True</span></code>, the linearly interpolating modes
(<cite>linear</cite>, <cite>bilinear</cite>, and <cite>trilinear</cite>) don’t proportionally align the
output and input pixels, and thus the output values can depend on the
input size. This was the default behavior for these modes up to version
0.3.1. Since then, the default behavior is <code class="docutils literal notranslate"><span class="pre">align_corners</span> <span class="pre">=</span> <span class="pre">False</span></code>.
See <a class="reference internal" href="nn.html#torch.nn.Upsample" title="torch.nn.Upsample"><code class="xref py py-class docutils literal notranslate"><span class="pre">Upsample</span></code></a> for concrete examples on how this
affects the outputs.</p>
</div>
</dd></dl>

</div>
<div class="section" id="upsample-nearest">
<h3><span class="hidden-section">upsample_nearest</span><a class="headerlink" href="#upsample-nearest" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample_nearest">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample_nearest</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample_nearest"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample_nearest" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input, using nearest neighbours’ pixel values.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with <code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='nearest')</span></code>.</p>
</div>
<p>Currently spatial and volumetric upsampling are supported (i.e. expected
inputs are 4 or 5 dimensional).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatia
size.</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – multiplier for spatial size. Has to be an integer.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in its backward pass that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
</dd></dl>

</div>
<div class="section" id="upsample-bilinear">
<h3><span class="hidden-section">upsample_bilinear</span><a class="headerlink" href="#upsample-bilinear" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.upsample_bilinear">
<code class="descclassname">torch.nn.functional.</code><code class="descname">upsample_bilinear</code><span class="sig-paren">(</span><em>input</em>, <em>size=None</em>, <em>scale_factor=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#upsample_bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.upsample_bilinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Upsamples the input, using bilinear upsampling.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function is deprecated in favor of <a class="reference internal" href="#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code></a>.
This is equivalent with
<code class="docutils literal notranslate"><span class="pre">nn.functional.interpolate(...,</span> <span class="pre">mode='bilinear',</span> <span class="pre">align_corners=True)</span></code>.</p>
</div>
<p>Expected inputs are spatial (4 dimensional). Use <cite>upsample_trilinear</cite> fo
volumetric (5 dimensional) inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – output spatial size.</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a><em>]</em>) – multiplier for spatial size</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in its backward pass that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
</dd></dl>

</div>
<div class="section" id="grid-sample">
<h3><span class="hidden-section">grid_sample</span><a class="headerlink" href="#grid-sample" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.grid_sample">
<code class="descclassname">torch.nn.functional.</code><code class="descname">grid_sample</code><span class="sig-paren">(</span><em>input</em>, <em>grid</em>, <em>mode='bilinear'</em>, <em>padding_mode='zeros'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#grid_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.grid_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and a flow-field <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code>, computes the
<code class="docutils literal notranslate"><span class="pre">output</span></code> using <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> values and pixel locations from <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code>.</p>
<p>Currently, only spatial (4-D) and volumetric (5-D) <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> are
supported.</p>
<p>In the spatial (4-D) case, for <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> with shape
<span class="math">\((N, C, H_\text{in}, W_\text{in})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> with shape
<span class="math">\((N, H_\text{out}, W_\text{out}, 2)\)</span>, the output will have shape
<span class="math">\((N, C, H_\text{out}, W_\text{out})\)</span>.</p>
<p>For each output location <code class="docutils literal notranslate"><span class="pre">output[n,</span> <span class="pre">:,</span> <span class="pre">h,</span> <span class="pre">w]</span></code>, the size-2 vector
<code class="docutils literal notranslate"><span class="pre">grid[n,</span> <span class="pre">h,</span> <span class="pre">w]</span></code> specifies <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> pixel locations <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>,
which are used to interpolate the output value <code class="docutils literal notranslate"><span class="pre">output[n,</span> <span class="pre">:,</span> <span class="pre">h,</span> <span class="pre">w]</span></code>.
In the case of 5D inputs, <code class="docutils literal notranslate"><span class="pre">grid[n,</span> <span class="pre">d,</span> <span class="pre">h,</span> <span class="pre">w]</span></code> specifies the
<code class="docutils literal notranslate"><span class="pre">x</span></code>, <code class="docutils literal notranslate"><span class="pre">y</span></code>, <code class="docutils literal notranslate"><span class="pre">z</span></code> pixel locations for interpolating
<code class="docutils literal notranslate"><span class="pre">output[n,</span> <span class="pre">:,</span> <span class="pre">d,</span> <span class="pre">h,</span> <span class="pre">w]</span></code>. <code class="xref py py-attr docutils literal notranslate"><span class="pre">mode</span></code> argument specifies <code class="docutils literal notranslate"><span class="pre">nearest</span></code> or
<code class="docutils literal notranslate"><span class="pre">bilinear</span></code> interpolation method to sample the input pixels.</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> specifies the sampling pixel locations normalized by the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> spatial dimensions. Therefore, it should have most values in
the range of <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code>. For example, values <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">-1,</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">-1</span></code> is the
left-top pixel of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>, and values  <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">y</span> <span class="pre">=</span> <span class="pre">1</span></code> is the
right-bottom pixel of <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">grid</span></code> has values outside the range of <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code>, the corresponding
outputs are handled as defined by <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding_mode</span></code>. Options are</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">padding_mode=&quot;zeros&quot;</span></code>: use <code class="docutils literal notranslate"><span class="pre">0</span></code> for out-of-bound grid locations,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">padding_mode=&quot;border&quot;</span></code>: use border values for out-of-bound grid locations,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">padding_mode=&quot;reflection&quot;</span></code>: use values at locations reflected by
the border for out-of-bound grid locations. For location far away
from the border, it will keep being reflected until becoming in bound,
e.g., (normalized) pixel location <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">-3.5</span></code> reflects by border <code class="docutils literal notranslate"><span class="pre">-1</span></code>
and becomes <code class="docutils literal notranslate"><span class="pre">x'</span> <span class="pre">=</span> <span class="pre">1.5</span></code>, then reflects by border <code class="docutils literal notranslate"><span class="pre">1</span></code> and becomes
<code class="docutils literal notranslate"><span class="pre">x''</span> <span class="pre">=</span> <span class="pre">-0.5</span></code>.</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is often used in building <a class="reference external" href="https://arxiv.org/abs/1506.02025">Spatial Transformer Networks</a> .</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the CUDA backend, this operation may induce nondeterministic
behaviour in its backward pass that is not easily switched off.
Please see the notes on <a class="reference internal" href="notes/randomness.html"><span class="doc">Reproducibility</span></a> for background.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input of shape <span class="math">\((N, C, H_\text{in}, W_\text{in})\)</span> (4-D case)
or <span class="math">\((N, C, D_\text{in}, H_\text{in}, W_\text{in})\)</span> (5-D case)</p></li>
<li><p><strong>grid</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – flow-field of shape <span class="math">\((N, H_\text{out}, W_\text{out}, 2)\)</span> (4-D case)
or <span class="math">\((N, D_\text{out}, H_\text{out}, W_\text{out}, 3)\)</span> (5-D case)</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – interpolation mode to calculate output values
<code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code> | <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'bilinear'</span></code></p></li>
<li><p><strong>padding_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – padding mode for outside grid values
<code class="docutils literal notranslate"><span class="pre">'zeros'</span></code> | <code class="docutils literal notranslate"><span class="pre">'border'</span></code> | <code class="docutils literal notranslate"><span class="pre">'reflection'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output Tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="affine-grid">
<h3><span class="hidden-section">affine_grid</span><a class="headerlink" href="#affine-grid" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.functional.affine_grid">
<code class="descclassname">torch.nn.functional.</code><code class="descname">affine_grid</code><span class="sig-paren">(</span><em>theta</em>, <em>size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/functional.html#affine_grid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.functional.affine_grid" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a 2d flow field, given a batch of affine matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">theta</span></code>.
Generally used in conjunction with <a class="reference internal" href="#torch.nn.functional.grid_sample" title="torch.nn.functional.grid_sample"><code class="xref py py-func docutils literal notranslate"><span class="pre">grid_sample()</span></code></a> to
implement Spatial Transformer Networks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>theta</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – input batch of affine matrices (<span class="math">\(N \times 2 \times 3\)</span>)</p></li>
<li><p><strong>size</strong> (<em>torch.Size</em>) – the target output image size (<span class="math">\(N \times C \times H \times W\)</span>).
Example: torch.Size((32, 3, 24, 24))</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output Tensor of size (<span class="math">\(N \times H \times W \times 2\)</span>)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>output (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">Tensor</a>)</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="dataparallel-functions-multi-gpu-distributed">
<h2>DataParallel functions (multi-GPU, distributed)<a class="headerlink" href="#dataparallel-functions-multi-gpu-distributed" title="Permalink to this headline">¶</a></h2>
<div class="section" id="data-parallel">
<h3><span class="hidden-section">data_parallel</span><a class="headerlink" href="#data-parallel" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="torch.nn.parallel.data_parallel">
<code class="descclassname">torch.nn.parallel.</code><code class="descname">data_parallel</code><span class="sig-paren">(</span><em>module</em>, <em>inputs</em>, <em>device_ids=None</em>, <em>output_device=None</em>, <em>dim=0</em>, <em>module_kwargs=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/data_parallel.html#data_parallel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.nn.parallel.data_parallel" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates module(input) in parallel across the GPUs given in device_ids.</p>
<p>This is the functional version of the DataParallel module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<a class="reference internal" href="nn.html#torch.nn.Module" title="torch.nn.Module"><em>Module</em></a>) – the module to evaluate in parallel</p></li>
<li><p><strong>inputs</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – inputs to the module</p></li>
<li><p><strong>device_ids</strong> (<em>list of python:int</em><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – GPU ids on which to replicate module</p></li>
<li><p><strong>output_device</strong> (<em>list of python:int</em><em> or </em><a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a>) – GPU location of the output  Use -1 to indicate the CPU.
(default: device_ids[0])</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor containing the result of module(input) located on
output_device</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tensors.html" class="btn btn-neutral float-right" title="torch.Tensor" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="nn.html" class="btn btn-neutral" title="torch.nn" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.nn.functional</a><ul>
<li><a class="reference internal" href="#convolution-functions">Convolution functions</a><ul>
<li><a class="reference internal" href="#conv1d"><span class="hidden-section">conv1d</span></a></li>
<li><a class="reference internal" href="#conv2d"><span class="hidden-section">conv2d</span></a></li>
<li><a class="reference internal" href="#conv3d"><span class="hidden-section">conv3d</span></a></li>
<li><a class="reference internal" href="#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li><a class="reference internal" href="#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li><a class="reference internal" href="#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
<li><a class="reference internal" href="#unfold"><span class="hidden-section">unfold</span></a></li>
<li><a class="reference internal" href="#fold"><span class="hidden-section">fold</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pooling-functions">Pooling functions</a><ul>
<li><a class="reference internal" href="#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li><a class="reference internal" href="#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li><a class="reference internal" href="#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li><a class="reference internal" href="#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li><a class="reference internal" href="#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li><a class="reference internal" href="#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li><a class="reference internal" href="#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li><a class="reference internal" href="#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li><a class="reference internal" href="#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li><a class="reference internal" href="#lp-pool1d"><span class="hidden-section">lp_pool1d</span></a></li>
<li><a class="reference internal" href="#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li><a class="reference internal" href="#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li><a class="reference internal" href="#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li><a class="reference internal" href="#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li><a class="reference internal" href="#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li><a class="reference internal" href="#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li><a class="reference internal" href="#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#non-linear-activation-functions">Non-linear activation functions</a><ul>
<li><a class="reference internal" href="#threshold"><span class="hidden-section">threshold</span></a></li>
<li><a class="reference internal" href="#relu"><span class="hidden-section">relu</span></a></li>
<li><a class="reference internal" href="#hardtanh"><span class="hidden-section">hardtanh</span></a></li>
<li><a class="reference internal" href="#relu6"><span class="hidden-section">relu6</span></a></li>
<li><a class="reference internal" href="#elu"><span class="hidden-section">elu</span></a></li>
<li><a class="reference internal" href="#selu"><span class="hidden-section">selu</span></a></li>
<li><a class="reference internal" href="#celu"><span class="hidden-section">celu</span></a></li>
<li><a class="reference internal" href="#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li><a class="reference internal" href="#prelu"><span class="hidden-section">prelu</span></a></li>
<li><a class="reference internal" href="#rrelu"><span class="hidden-section">rrelu</span></a></li>
<li><a class="reference internal" href="#glu"><span class="hidden-section">glu</span></a></li>
<li><a class="reference internal" href="#gelu"><span class="hidden-section">gelu</span></a></li>
<li><a class="reference internal" href="#logsigmoid"><span class="hidden-section">logsigmoid</span></a></li>
<li><a class="reference internal" href="#hardshrink"><span class="hidden-section">hardshrink</span></a></li>
<li><a class="reference internal" href="#tanhshrink"><span class="hidden-section">tanhshrink</span></a></li>
<li><a class="reference internal" href="#softsign"><span class="hidden-section">softsign</span></a></li>
<li><a class="reference internal" href="#softplus"><span class="hidden-section">softplus</span></a></li>
<li><a class="reference internal" href="#softmin"><span class="hidden-section">softmin</span></a></li>
<li><a class="reference internal" href="#softmax"><span class="hidden-section">softmax</span></a></li>
<li><a class="reference internal" href="#softshrink"><span class="hidden-section">softshrink</span></a></li>
<li><a class="reference internal" href="#gumbel-softmax"><span class="hidden-section">gumbel_softmax</span></a></li>
<li><a class="reference internal" href="#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li><a class="reference internal" href="#tanh"><span class="hidden-section">tanh</span></a></li>
<li><a class="reference internal" href="#sigmoid"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#normalization-functions">Normalization functions</a><ul>
<li><a class="reference internal" href="#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li><a class="reference internal" href="#instance-norm"><span class="hidden-section">instance_norm</span></a></li>
<li><a class="reference internal" href="#layer-norm"><span class="hidden-section">layer_norm</span></a></li>
<li><a class="reference internal" href="#local-response-norm"><span class="hidden-section">local_response_norm</span></a></li>
<li><a class="reference internal" href="#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#linear-functions">Linear functions</a><ul>
<li><a class="reference internal" href="#linear"><span class="hidden-section">linear</span></a></li>
<li><a class="reference internal" href="#bilinear"><span class="hidden-section">bilinear</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#dropout-functions">Dropout functions</a><ul>
<li><a class="reference internal" href="#dropout"><span class="hidden-section">dropout</span></a></li>
<li><a class="reference internal" href="#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li><a class="reference internal" href="#dropout2d"><span class="hidden-section">dropout2d</span></a></li>
<li><a class="reference internal" href="#dropout3d"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#sparse-functions">Sparse functions</a><ul>
<li><a class="reference internal" href="#embedding"><span class="hidden-section">embedding</span></a></li>
<li><a class="reference internal" href="#embedding-bag"><span class="hidden-section">embedding_bag</span></a></li>
<li><a class="reference internal" href="#one-hot"><span class="hidden-section">one_hot</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#distance-functions">Distance functions</a><ul>
<li><a class="reference internal" href="#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li><a class="reference internal" href="#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
<li><a class="reference internal" href="#pdist"><span class="hidden-section">pdist</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#loss-functions">Loss functions</a><ul>
<li><a class="reference internal" href="#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li><a class="reference internal" href="#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li><a class="reference internal" href="#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li><a class="reference internal" href="#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li><a class="reference internal" href="#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li><a class="reference internal" href="#ctc-loss"><span class="hidden-section">ctc_loss</span></a></li>
<li><a class="reference internal" href="#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li><a class="reference internal" href="#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li><a class="reference internal" href="#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li><a class="reference internal" href="#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li><a class="reference internal" href="#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li><a class="reference internal" href="#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li><a class="reference internal" href="#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li><a class="reference internal" href="#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li><a class="reference internal" href="#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li><a class="reference internal" href="#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li><a class="reference internal" href="#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li><a class="reference internal" href="#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#vision-functions">Vision functions</a><ul>
<li><a class="reference internal" href="#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li><a class="reference internal" href="#pad"><span class="hidden-section">pad</span></a></li>
<li><a class="reference internal" href="#interpolate"><span class="hidden-section">interpolate</span></a></li>
<li><a class="reference internal" href="#upsample"><span class="hidden-section">upsample</span></a></li>
<li><a class="reference internal" href="#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li><a class="reference internal" href="#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li><a class="reference internal" href="#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li><a class="reference internal" href="#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
<li><a class="reference internal" href="#dataparallel-functions-multi-gpu-distributed">DataParallel functions (multi-GPU, distributed)</a><ul>
<li><a class="reference internal" href="#data-parallel"><span class="hidden-section">data_parallel</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script type="text/javascript" src="_static/jquery.js"></script>
         <script type="text/javascript" src="_static/underscore.js"></script>
         <script type="text/javascript" src="_static/doctools.js"></script>
         <script type="text/javascript" src="_static/language_data.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js"></script>
         <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js"></script>
         <script type="text/javascript" src="_static/katex_autorenderer.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://pytorch.org/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>